<details>
  <summary><b>TOC</b></summary>
  <ol>
    <li><a href=#visual-localization>Visual Localization</a></li>
      <ul>
        <li><a href=#Ultrafast-4D-scanning-transmission-electron-microscopy-for-imaging-of-localized-optical-fields>Ultrafast 4D scanning transmission electron microscopy for imaging of localized optical fields</a></li>
        <li><a href=#Generative-Ghost:-Investigating-Ranking-Bias-Hidden-in-AI-Generated-Videos>Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos</a></li>
        <li><a href=#PDV:-Prompt-Directional-Vectors-for-Zero-shot-Composed-Image-Retrieval>PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval</a></li>
        <li><a href=#AstroLoc:-Robust-Space-to-Ground-Image-Localizer>AstroLoc: Robust Space to Ground Image Localizer</a></li>
      </ul>
    </li>
    <li><a href=#nerf>NeRF</a></li>
      <ul>
        <li><a href=#PrismAvatar:-Real-time-animated-3D-neural-head-avatars-on-edge-devices>PrismAvatar: Real-time animated 3D neural head avatars on edge devices</a></li>
        <li><a href=#Grounding-Creativity-in-Physics:-A-Brief-Survey-of-Physical-Priors-in-AIGC>Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC</a></li>
      </ul>
    </li>
  </ol>
</details>

## Visual Localization  

### [Ultrafast 4D scanning transmission electron microscopy for imaging of localized optical fields](http://arxiv.org/abs/2502.07338)  
Petr Koutenský, Neli Laštovičková Streshkova, Kamila Moriová, Marius Constantin Chirita Mihaila, Daniel Burda, Alexandr Knápek, Martin Kozák  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Ultrafast electron microscopy aims for imaging transient phenomena occurring on nanoscale. One of its goals is to visualize localized optical and plasmonic modes generated by coherent excitation in the vicinity of various types of nanostructures. Such imaging capability was enabled by photon-induced near-field optical microscopy, which is based on spectral filtering of electrons inelastically scattered due to the stimulated interaction with the near-field. Here we report on the development of ultrafast 4D scanning transmission electron microscopy, which allows us to image the transverse components of the optical near-field while avoiding the need of electron spectral filtering. We demonstrate that this method is capable of imaging optical near-fields of a tungsten nanotip and ponderomotive potential of an optical standing wave with a spatial resolution of 21 nm.  
  </ol>  
</details>  
**comments**: v1: preprint; licence: CC BY 4.0  
  
### [Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos](http://arxiv.org/abs/2502.07327)  
Haowen Gao, Liang Pang, Shicheng Xu, Leigang Qu, Tat-Seng Chua, Huawei Shen, Xueqi Cheng  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    With the rapid development of AI-generated content (AIGC), the creation of high-quality AI-generated videos has become faster and easier, resulting in the Internet being flooded with all kinds of video content. However, the impact of these videos on the content ecosystem remains largely unexplored. Video information retrieval remains a fundamental approach for accessing video content. Building on the observation that retrieval models often favor AI-generated content in ad-hoc and image retrieval tasks, we investigate whether similar biases emerge in the context of challenging video retrieval, where temporal and visual factors may further influence model behavior. To explore this, we first construct a comprehensive benchmark dataset containing both real and AI-generated videos, along with a set of fair and rigorous metrics to assess bias. This benchmark consists of 13,000 videos generated by two state-of-the-art open-source video generation models. We meticulously design a suite of rigorous metrics to accurately measure this preference, accounting for potential biases arising from the limited frame rate and suboptimal quality of AIGC videos. We then applied three off-the-shelf video retrieval models to perform retrieval tasks on this hybrid dataset. Our findings reveal a clear preference for AI-generated videos in retrieval. Further investigation shows that incorporating AI-generated videos into the training set of retrieval models exacerbates this bias. Unlike the preference observed in image modalities, we find that video retrieval bias arises from both unseen visual and temporal information, making the root causes of video bias a complex interplay of these two factors. To mitigate this bias, we fine-tune the retrieval models using a contrastive learning approach. The results of this study highlight the potential implications of AI-generated videos on retrieval systems.  
  </ol>  
</details>  
  
### [PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval](http://arxiv.org/abs/2502.07215)  
Osman Tursun, Sinan Kalkan, Simon Denman, Clinton Fookes  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Zero-shot composed image retrieval (ZS-CIR) enables image search using a reference image and text prompt without requiring specialized text-image composition networks trained on large-scale paired data. However, current ZS-CIR approaches face three critical limitations in their reliance on composed text embeddings: static query embedding representations, insufficient utilization of image embeddings, and suboptimal performance when fusing text and image embeddings. To address these challenges, we introduce the Prompt Directional Vector (PDV), a simple yet effective training-free enhancement that captures semantic modifications induced by user prompts. PDV enables three key improvements: (1) dynamic composed text embeddings where prompt adjustments are controllable via a scaling factor, (2) composed image embeddings through semantic transfer from text prompts to image features, and (3) weighted fusion of composed text and image embeddings that enhances retrieval by balancing visual and semantic similarity. Our approach serves as a plug-and-play enhancement for existing ZS-CIR methods with minimal computational overhead. Extensive experiments across multiple benchmarks demonstrate that PDV consistently improves retrieval performance when integrated with state-of-the-art ZS-CIR approaches, particularly for methods that generate accurate compositional embeddings. The code will be publicly available.  
  </ol>  
</details>  
  
### [AstroLoc: Robust Space to Ground Image Localizer](http://arxiv.org/abs/2502.07003)  
Gabriele Berton, Alex Stoken, Carlo Masone  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Astronauts take thousands of photos of Earth per day from the International Space Station, which, once localized on Earth's surface, are used for a multitude of tasks, ranging from climate change research to disaster management. The localization process, which has been performed manually for decades, has recently been approached through image retrieval solutions: given an astronaut photo, find its most similar match among a large database of geo-tagged satellite images, in a task called Astronaut Photography Localization (APL). Yet, existing APL approaches are trained only using satellite images, without taking advantage of the millions open-source astronaut photos. In this work we present the first APL pipeline capable of leveraging astronaut photos for training. We first produce full localization information for 300,000 manually weakly labeled astronaut photos through an automated pipeline, and then use these images to train a model, called AstroLoc. AstroLoc learns a robust representation of Earth's surface features through two losses: astronaut photos paired with their matching satellite counterparts in a pairwise loss, and a second loss on clusters of satellite imagery weighted by their relevance to astronaut photography via unsupervised mining. We find that AstroLoc achieves a staggering 35% average improvement in recall@1 over previous SOTA, pushing the limits of existing datasets with a recall@100 consistently over 99%. Finally, we note that AstroLoc, without any fine-tuning, provides excellent results for related tasks like the lost-in-space satellite problem and historical space imagery localization.  
  </ol>  
</details>  
  
  



## NeRF  

### [PrismAvatar: Real-time animated 3D neural head avatars on edge devices](http://arxiv.org/abs/2502.07030)  
Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We present PrismAvatar: a 3D head avatar model which is designed specifically to enable real-time animation and rendering on resource-constrained edge devices, while still enjoying the benefits of neural volumetric rendering at training time. By integrating a rigged prism lattice with a 3D morphable head model, we use a hybrid rendering model to simultaneously reconstruct a mesh-based head and a deformable NeRF model for regions not represented by the 3DMM. We then distill the deformable NeRF into a rigged mesh and neural textures, which can be animated and rendered efficiently within the constraints of the traditional triangle rendering pipeline. In addition to running at 60 fps with low memory usage on mobile devices, we find that our trained models have comparable quality to state-of-the-art 3D avatar models on desktop devices.  
  </ol>  
</details>  
**comments**: 8 pages, 5 figures  
  
### [Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC](http://arxiv.org/abs/2502.07007)  
Siwei Meng, Yawei Luo, Ping Liu  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation.  
  </ol>  
</details>  
  
  



