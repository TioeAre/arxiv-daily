**TOC**  
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#slam>SLAM</a></li>
      <ul>
        <li><a href=#iMatching:-Imperative-Correspondence-Learning>iMatching: Imperative Correspondence Learning</a></li>
        <li><a href=#Event-based-Visual-Inertial-Velometer>Event-based Visual Inertial Velometer</a></li>
        <li><a href=#CoVOR-SLAM:-Cooperative-SLAM-using-Visual-Odometry-and-Ranges-for-Multi-Robot-Systems>CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems</a></li>
        <li><a href=#Dense-Visual-Odometry-Using-Genetic-Algorithm>Dense Visual Odometry Using Genetic Algorithm</a></li>
        <li><a href=#Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry/SLAM>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</a></li>
        <li><a href=#Converting-Depth-Images-and-Point-Clouds-for-Feature-based-Pose-Estimation>Converting Depth Images and Point Clouds for Feature-based Pose Estimation</a></li>
        <li><a href=#Open-Structure:-a-Structural-Benchmark-Dataset-for-SLAM-Algorithms>Open-Structure: a Structural Benchmark Dataset for SLAM Algorithms</a></li>
        <li><a href=#Jointly-Optimized-Global-Local-Visual-Localization-of-UAVs>Jointly Optimized Global-Local Visual Localization of UAVs</a></li>
        <li><a href=#l-dyno:-framework-to-learn-consistent-visual-features-using-robot's-motion>l-dyno: framework to learn consistent visual features using robot's motion</a></li>
        <li><a href=#XVO:-Generalized-Visual-Odometry-via-Cross-Modal-Self-Training>XVO: Generalized Visual Odometry via Cross-Modal Self-Training</a></li>
      </ul>
    </li>
    <li><a href=#sfm>SFM</a></li>
      <ul>
        <li><a href=#Visual-Geometry-Grounded-Deep-Structure-From-Motion>Visual Geometry Grounded Deep Structure From Motion</a></li>
        <li><a href=#Distributed-Global-Structure-from-Motion-with-a-Deep-Front-End>Distributed Global Structure-from-Motion with a Deep Front-End</a></li>
        <li><a href=#Robot-Hand-Eye-Calibration-using-Structure-from-Motion>Robot Hand-Eye Calibration using Structure-from-Motion</a></li>
        <li><a href=#LOSTU:-Fast,-Scalable,-and-Uncertainty-Aware-Triangulation>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</a></li>
        <li><a href=#MonoProb:-Self-Supervised-Monocular-Depth-Estimation-with-Interpretable-Uncertainty>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</a></li>
        <li><a href=#VET:-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</a></li>
        <li><a href=#A-Quantitative-Evaluation-of-Dense-3D-Reconstruction-of-Sinus-Anatomy-from-Monocular-Endoscopic-Video>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</a></li>
        <li><a href=#FMRT:-Learning-Accurate-Feature-Matching-with-Reconciliatory-Transformer>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</a></li>
        <li><a href=#Colmap-PCD:-An-Open-source-Tool-for-Fine-Image-to-point-cloud-Registration>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</a></li>
        <li><a href=#LocoNeRF:-A-NeRF-based-Approach-for-Local-Structure-from-Motion-for-Precise-Localization>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</a></li>
      </ul>
    </li>
    <li><a href=#visual-localization>Visual Localization</a></li>
      <ul>
        <li><a href=#Lite-Mind:-Towards-Efficient-and-Versatile-Brain-Representation-Network>Lite-Mind: Towards Efficient and Versatile Brain Representation Network</a></li>
        <li><a href=#FreestyleRet:-Retrieving-Images-from-Style-Diversified-Queries>FreestyleRet: Retrieving Images from Style-Diversified Queries</a></li>
        <li><a href=#Implicit-Learning-of-Scene-Geometry-from-Poses-for-Global-Localization>Implicit Learning of Scene Geometry from Poses for Global Localization</a></li>
        <li><a href=#Language-only-Efficient-Training-of-Zero-shot-Composed-Image-Retrieval>Language-only Efficient Training of Zero-shot Composed Image Retrieval</a></li>
        <li><a href=#G2D:-From-Global-to-Dense-Radiography-Representation-Learning-via-Vision-Language-Pre-training>G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training</a></li>
        <li><a href=#Improve-Supervised-Representation-Learning-with-Masked-Image-Modeling>Improve Supervised Representation Learning with Masked Image Modeling</a></li>
        <li><a href=#Grounding-Everything:-Emerging-Localization-Properties-in-Vision-Language-Transformers>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</a></li>
        <li><a href=#Global-Localization:-Utilizing-Relative-Spatio-Temporal-Geometric-Constraints-from-Adjacent-and-Distant-Cameras>Global Localization: Utilizing Relative Spatio-Temporal Geometric Constraints from Adjacent and Distant Cameras</a></li>
        <li><a href=#HKUST-at-SemEval-2023-Task-1:-Visual-Word-Sense-Disambiguation-with-Context-Augmentation-and-Visual-Assistance>HKUST at SemEval-2023 Task 1: Visual Word Sense Disambiguation with Context Augmentation and Visual Assistance</a></li>
        <li><a href=#Label-efficient-Training-of-Small-Task-specific-Models-by-Leveraging-Vision-Foundation-Models>Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models</a></li>
      </ul>
    </li>
    <li><a href=#keypoint-detection>Keypoint Detection</a></li>
      <ul>
        <li><a href=#Tracking-Object-Positions-in-Reinforcement-Learning:-A-Metric-for-Keypoint-Detection-(extended-version)>Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version)</a></li>
        <li><a href=#Utilizing-Radiomic-Feature-Analysis-For-Automated-MRI-Keypoint-Detection:-Enhancing-Graph-Applications>Utilizing Radiomic Feature Analysis For Automated MRI Keypoint Detection: Enhancing Graph Applications</a></li>
        <li><a href=#Back-to-3D:-Few-Shot-3D-Keypoint-Detection-with-Back-Projected-2D-Features>Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features</a></li>
        <li><a href=#Diffusion-3D-Features-(Diff3F):-Decorating-Untextured-Shapes-with-Distilled-Semantic-Features>Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features</a></li>
        <li><a href=#Riemannian-Self-Attention-Mechanism-for-SPD-Networks>Riemannian Self-Attention Mechanism for SPD Networks</a></li>
        <li><a href=#A-manometric-feature-descriptor-with-linear-SVM-to-distinguish-esophageal-contraction-vigor>A manometric feature descriptor with linear-SVM to distinguish esophageal contraction vigor</a></li>
        <li><a href=#Instance-aware-3D-Semantic-Segmentation-powered-by-Shape-Generators-and-Classifiers>Instance-aware 3D Semantic Segmentation powered by Shape Generators and Classifiers</a></li>
        <li><a href=#CurriculumLoc:-Enhancing-Cross-Domain-Geolocalization-through-Multi-Stage-Refinement>CurriculumLoc: Enhancing Cross-Domain Geolocalization through Multi-Stage Refinement</a></li>
        <li><a href=#Video-based-Sequential-Bayesian-Homography-Estimation-for-Soccer-Field-Registration>Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration</a></li>
        <li><a href=#Processing-and-Segmentation-of-Human-Teeth-from-2D-Images-using-Weakly-Supervised-Learning>Processing and Segmentation of Human Teeth from 2D Images using Weakly Supervised Learning</a></li>
      </ul>
    </li>
    <li><a href=#image-matching>Image Matching</a></li>
      <ul>
        <li><a href=#Visual-Geometry-Grounded-Deep-Structure-From-Motion>Visual Geometry Grounded Deep Structure From Motion</a></li>
        <li><a href=#Steerers:-A-framework-for-rotation-equivariant-keypoint-descriptors>Steerers: A framework for rotation equivariant keypoint descriptors</a></li>
        <li><a href=#DSeg:-Direct-Line-Segments-Detection>DSeg: Direct Line Segments Detection</a></li>
        <li><a href=#Utilizing-Radiomic-Feature-Analysis-For-Automated-MRI-Keypoint-Detection:-Enhancing-Graph-Applications>Utilizing Radiomic Feature Analysis For Automated MRI Keypoint Detection: Enhancing Graph Applications</a></li>
        <li><a href=#LGFCTR:-Local-and-Global-Feature-Convolutional-Transformer-for-Image-Matching>LGFCTR: Local and Global Feature Convolutional Transformer for Image Matching</a></li>
        <li><a href=#Zero-shot-Translation-of-Attention-Patterns-in-VQA-Models-to-Natural-Language>Zero-shot Translation of Attention Patterns in VQA Models to Natural Language</a></li>
        <li><a href=#An-invariant-feature-extraction-for-multi-modal-images-matching>An invariant feature extraction for multi-modal images matching</a></li>
        <li><a href=#RD-VIO:-Robust-Visual-Inertial-Odometry-for-Mobile-Augmented-Reality-in-Dynamic-Environments>RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments</a></li>
        <li><a href=#Player-Re-Identification-Using-Body-Part-Appearences>Player Re-Identification Using Body Part Appearences</a></li>
        <li><a href=#FMRT:-Learning-Accurate-Feature-Matching-with-Reconciliatory-Transformer>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</a></li>
      </ul>
    </li>
    <li><a href=#nerf>NeRF</a></li>
      <ul>
        <li><a href=#MuRF:-Multi-Baseline-Radiance-Fields>MuRF: Multi-Baseline Radiance Fields</a></li>
        <li><a href=#EAGLES:-Efficient-Accelerated-3D-Gaussians-with-Lightweight-EncodingS>EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS</a></li>
        <li><a href=#Correspondences-of-the-Third-Kind:-Camera-Pose-Estimation-from-Object-Reflection>Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection</a></li>
        <li><a href=#Multi-View-Unsupervised-Image-Generation-with-Cross-Attention-Guidance>Multi-View Unsupervised Image Generation with Cross Attention Guidance</a></li>
        <li><a href=#Towards-4D-Human-Video-Stylization>Towards 4D Human Video Stylization</a></li>
        <li><a href=#Identity-Obscured-Neural-Radiance-Fields:-Privacy-Preserving-3D-Facial-Reconstruction>Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial Reconstruction</a></li>
        <li><a href=#Inpaint3D:-3D-Scene-Content-Generation-using-2D-Inpainting-Diffusion>Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion</a></li>
        <li><a href=#Gaussian-Flow:-4D-Reconstruction-with-Dynamic-3D-Gaussian-Particle>Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</a></li>
        <li><a href=#Artist-Friendly-Relightable-and-Animatable-Neural-Heads>Artist-Friendly Relightable and Animatable Neural Heads</a></li>
        <li><a href=#Evaluating-the-point-cloud-of-individual-trees-generated-from-images-based-on-Neural-Radiance-fields-(NeRF)-method>Evaluating the point cloud of individual trees generated from images based on Neural Radiance fields (NeRF) method</a></li>
      </ul>
    </li>
  </ol>
</details>

## SLAM  

### [iMatching: Imperative Correspondence Learning](http://arxiv.org/abs/2312.02141)  
Zitong Zhan, Dasong Gao, Yun-Jou Lin, Youjie Xia, Chen Wang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction. Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels. To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence. It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning. Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model. To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment. Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models.  
  </ol>  
</details>  
  
### [Event-based Visual Inertial Velometer](http://arxiv.org/abs/2311.18189)  
Xiuyuan Lu, Yi Zhou, Shaojie Shen  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Neuromorphic event-based cameras are bio-inspired visual sensors with asynchronous pixels and extremely high temporal resolution. Such favorable properties make them an excellent choice for solving state estimation tasks under aggressive ego motion. However, failures of camera pose tracking are frequently witnessed in state-of-the-art event-based visual odometry systems when the local map cannot be updated in time. One of the biggest roadblocks for this specific field is the absence of efficient and robust methods for data association without imposing any assumption on the environment. This problem seems, however, unlikely to be addressed as in standard vision due to the motion-dependent observability of event data. Therefore, we propose a mapping-free design for event-based visual-inertial state estimation in this paper. Instead of estimating the position of the event camera, we find that recovering the instantaneous linear velocity is more consistent with the differential working principle of event cameras. The proposed event-based visual-inertial velometer leverages a continuous-time formulation that incrementally fuses the heterogeneous measurements from a stereo event camera and an inertial measurement unit. Experiments on the synthetic dataset demonstrate that the proposed method can recover instantaneous linear velocity in metric scale with low latency.  
  </ol>  
</details>  
  
### [CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems](http://arxiv.org/abs/2311.12580)  
Young-Hee Lee, Chen Zhu, Thomas Wiedemann, Emanuel Staudinger, Siwei Zhang, Christoph Günther  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    A swarm of robots has advantages over a single robot, since it can explore larger areas much faster and is more robust to single-point failures. Accurate relative positioning is necessary to successfully carry out a collaborative mission without collisions. When Visual Simultaneous Localization and Mapping (VSLAM) is used to estimate the poses of each robot, inter-agent loop closing is widely applied to reduce the relative positioning errors. This technique can mitigate errors using the feature points commonly observed by different robots. However, it requires significant computing and communication capabilities to detect inter-agent loops, and to process the data transmitted by multiple agents. In this paper, we propose Collaborative SLAM using Visual Odometry and Range measurements (CoVOR-SLAM) to overcome this challenge. In the framework of CoVOR-SLAM, robots only need to exchange pose estimates, covariances (uncertainty) of the estimates, and range measurements between robots. Since CoVOR-SLAM does not require to associate visual features and map points observed by different agents, the computational and communication loads are significantly reduced. The required range measurements can be obtained using pilot signals of the communication system, without requiring complex additional infrastructure. We tested CoVOR-SLAM using real images as well as real ultra-wideband-based ranges obtained with two rovers. In addition, CoVOR-SLAM is evaluated with a larger scale multi-agent setup exploiting public image datasets and ranges generated using a realistic simulation. The results show that CoVOR-SLAM can accurately estimate the robots' poses, requiring much less computational power and communication capabilities than the inter-agent loop closing technique.  
  </ol>  
</details>  
**comments**: Submitted to the IEEE Transactions on Intelligent Transportation
  Systems  
  
### [Dense Visual Odometry Using Genetic Algorithm](http://arxiv.org/abs/2311.06149)  
Slimane Djema, Zoubir Abdeslem Benselama, Ramdane Hedjar, Krabi Abdallah  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Our work aims to estimate the camera motion mounted on the head of a mobile robot or a moving object from RGB-D images in a static scene. The problem of motion estimation is transformed into a nonlinear least squares function. Methods for solving such problems are iterative. Various classic methods gave an iterative solution by linearizing this function. We can also use the metaheuristic optimization method to solve this problem and improve results. In this paper, a new algorithm is developed for visual odometry using a sequence of RGB-D images. This algorithm is based on a genetic algorithm. The proposed iterative genetic algorithm searches using particles to estimate the optimal motion and then compares it to the traditional methods. To evaluate our method, we use the root mean square error to compare it with the based energy method and another metaheuristic method. We prove the efficiency of our innovative algorithm on a large set of images.  
  </ol>  
</details>  
**comments**: 9 pages, 9 figures  
  
### [Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM](http://arxiv.org/abs/2311.03722)  
Seongwook Yoon, Jaehyun Kim, Sanghoon Sull  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.  
  </ol>  
</details>  
**comments**: 12 pages  
  
### [Converting Depth Images and Point Clouds for Feature-based Pose Estimation](http://arxiv.org/abs/2310.14924)  
[[code](https://github.com/rlsch/depth-conversions)]  
Robert Lösch, Mark Sastuba, Jonas Toth, Bernhard Jung  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    In recent years, depth sensors have become more and more affordable and have found their way into a growing amount of robotic systems. However, mono- or multi-modal sensor registration, often a necessary step for further processing, faces many challenges on raw depth images or point clouds. This paper presents a method of converting depth data into images capable of visualizing spatial details that are basically hidden in traditional depth images. After noise removal, a neighborhood of points forms two normal vectors whose difference is encoded into this new conversion. Compared to Bearing Angle images, our method yields brighter, higher-contrast images with more visible contours and more details. We tested feature-based pose estimation of both conversions in a visual odometry task and RGB-D SLAM. For all tested features, AKAZE, ORB, SIFT, and SURF, our new Flexion images yield better results than Bearing Angle images and show great potential to bridge the gap between depth data and classical computer vision. Source code is available here: https://rlsch.github.io/depth-flexion-conversion.  
  </ol>  
</details>  
**comments**: to be published in IROS 2023 conference proceedings  
  
### [Open-Structure: a Structural Benchmark Dataset for SLAM Algorithms](http://arxiv.org/abs/2310.10931)  
[[code](https://github.com/yanyan-li/open-structure)]  
Yanyan Li, Zhao Guo, Ze Yang, Yanbiao Sun, Liang Zhao, Federico Tombari  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    This paper introduces a new benchmark dataset, Open-Structure, for evaluating visual odometry and SLAM methods, which directly equips point and line measurements, correspondences, structural associations, and co-visibility factor graphs instead of providing raw images. Based on the proposed benchmark dataset, these 2D or 3D data can be directly input to different stages of SLAM pipelines to avoid the impact of the data preprocessing modules in ablation experiments. First, we propose a dataset generator for real-world and simulated scenarios. In real-world scenes, it maintains the same observations and occlusions as actual feature extraction results. Those generated simulation sequences enhance the dataset's diversity by introducing various carefully designed trajectories and observations. Second, a SLAM baseline is proposed using our dataset to evaluate widely used modules in camera pose tracking, parametrization, and optimization modules. By evaluating these state-of-the-art algorithms across different scenarios, we discern each module's strengths and weaknesses within the camera tracking and optimization process. Our dataset and baseline are available at \url{https://github.com/yanyan-li/Open-Structure}.  
  </ol>  
</details>  
  
### [Jointly Optimized Global-Local Visual Localization of UAVs](http://arxiv.org/abs/2310.08082)  
Haoling Li, Jiuniu Wang, Zhiwei Wei, Wenjia Xu  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Navigation and localization of UAVs present a challenge when global navigation satellite systems (GNSS) are disrupted and unreliable. Traditional techniques, such as simultaneous localization and mapping (SLAM) and visual odometry (VO), exhibit certain limitations in furnishing absolute coordinates and mitigating error accumulation. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching with ortho satellite images. However, doing so cannot guarantee real-time performance due to the complex matching process. To address these challenges, we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL network is a two-stage visual localization approach, combining a large-scale retrieval module that finds similar regions with the UAV flight scene, and a fine-grained matching module that localizes the precise UAV coordinate, enabling real-time and precise localization. The training process is jointly optimized in an end-to-end manner to further enhance the model capability. Experiments on six UAV flight scenes encompassing both texture-rich and texture-sparse regions demonstrate the ability of our model to achieve the real-time precise localization requirements of UAVs. Particularly, our method achieves a localization error of only 2.39 meters in 0.48 seconds in a village scene with sparse texture features.  
  </ol>  
</details>  
  
### [l-dyno: framework to learn consistent visual features using robot's motion](http://arxiv.org/abs/2310.06249)  
Kartikeya Singh, Charuvaran Adhivarahan, Karthik Dantu  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Historically, feature-based approaches have been used extensively for camera-based robot perception tasks such as localization, mapping, tracking, and others. Several of these approaches also combine other sensors (inertial sensing, for example) to perform combined state estimation. Our work rethinks this approach; we present a representation learning mechanism that identifies visual features that best correspond to robot motion as estimated by an external signal. Specifically, we utilize the robot's transformations through an external signal (inertial sensing, for example) and give attention to image space that is most consistent with the external signal. We use a pairwise consistency metric as a representation to keep the visual features consistent through a sequence with the robot's relative pose transformations. This approach enables us to incorporate information from the robot's perspective instead of solely relying on the image attributes. We evaluate our approach on real-world datasets such as KITTI & EuRoC and compare the refined features with existing feature descriptors. We also evaluate our method using our real robot experiment. We notice an average of 49% reduction in the image search space without compromising the trajectory estimation accuracy. Our method reduces the execution time of visual odometry by 4.3% and also reduces reprojection errors. We demonstrate the need to select only the most important features and show the competitiveness using various feature detection baselines.  
  </ol>  
</details>  
**comments**: 7 pages, 6 figures  
  
### [XVO: Generalized Visual Odometry via Cross-Modal Self-Training](http://arxiv.org/abs/2309.16772)  
Lei Lai, Zhongkai Shangguan, Jimuyang Zhang, Eshed Ohn-Bar  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to significantly enhance the semi-supervised learning process while alleviating noisy pseudo-labels, particularly in highly dynamic and out-of-domain video data. Our proposed teacher network achieves state-of-the-art performance on the commonly used KITTI benchmark despite no multi-frame optimization or knowledge of camera parameters. Combined with the proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge transfer across diverse conditions on KITTI, nuScenes, and Argoverse without fine-tuning.  
  </ol>  
</details>  
**comments**: ICCV 2023, Paris https://genxvo.github.io/  
  
  



## SFM  

### [Visual Geometry Grounded Deep Structure From Motion](http://arxiv.org/abs/2312.04563)  
Jianyuan Wang, Nikita Karaev, Christian Rupprecht, David Novotny  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.  
  </ol>  
</details>  
**comments**: 8 figures. Project page: https://vggsfm.github.io/  
  
### [Distributed Global Structure-from-Motion with a Deep Front-End](http://arxiv.org/abs/2311.18801)  
[[code](https://github.com/borglab/gtsfm)]  
Ayush Baid, John Lambert, Travis Driver, Akshay Krishnan, Hayk Stepanyan, Frank Dellaert  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.  
  </ol>  
</details>  
  
### [Robot Hand-Eye Calibration using Structure-from-Motion](http://arxiv.org/abs/2311.11808)  
Nicolas Andreff, Radu Horaud, Bernard Espiau  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.  
  </ol>  
</details>  
  
### [LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation](http://arxiv.org/abs/2311.11171)  
Sébastien Henry, John A. Christian  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Triangulation algorithms often aim to minimize the reprojection ( $L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.  
  </ol>  
</details>  
**comments**: 11 pages, 6 figures, 3 tables  
  
### [MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty](http://arxiv.org/abs/2311.06137)  
[[code](https://github.com/cea-list/monoprob)]  
Rémi Marsal, Florian Chabot, Angelique Loesch, William Grolleau, Hichem Sahbi  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb  
  </ol>  
</details>  
**comments**: Accepted at WACV 2024  
  
### [VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering](http://arxiv.org/abs/2311.04634)  
[[code](https://github.com/lfranke/vet)]  
Linus Franke, Darius Rückert, Laura Fink, Matthias Innmann, Marc Stamminger  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.  
  </ol>  
</details>  
  
### [A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video](http://arxiv.org/abs/2310.14364)  
Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Isabela Hernández, Jonas Winter, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.  
  </ol>  
</details>  
  
### [FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer](http://arxiv.org/abs/2310.13605)  
Xinyu Zhang, Li Wang, Zhiqiang Jiang, Kun Dai, Tao Xie, Lei Yang, Wenhao Yu, Yang Shen, Jun Li  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.  
  </ol>  
</details>  
  
### [Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration](http://arxiv.org/abs/2310.05504)  
[[code](https://github.com/xiaobaiiiiii/colmap-pcd)]  
Chunge Bai, Ruijie Fu, Xiang Gao  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD ${^{3}}$ , an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.  
  </ol>  
</details>  
  
### [LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization](http://arxiv.org/abs/2310.05134)  
Artem Nenashev, Mikhail Kurenkov, Andrei Potapov, Iana Zhura, Maksim Katerishich, Dzmitry Tsetserukou  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.  
  </ol>  
</details>  
  
  



## Visual Localization  

### [Lite-Mind: Towards Efficient and Versatile Brain Representation Network](http://arxiv.org/abs/2312.03781)  
Zixuan Gong, Qi Zhang, Duoqian Miao, Guangyin Bao, Liang Hu  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind, a lightweight, efficient, and versatile brain representation network based on discrete Fourier transform, that efficiently aligns fMRI voxels to fine-grained information of CLIP. Our experiments demonstrate that Lite-Mind achieves an impressive 94.3% fMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind is also proven to be able to be migrated to smaller brain datasets and establishes a new state-of-the-art for zero-shot classification on the GOD dataset. The code is available at https://github.com/gongzix/Lite-Mind.  
  </ol>  
</details>  
  
### [FreestyleRet: Retrieving Images from Style-Diversified Queries](http://arxiv.org/abs/2312.02428)  
Hao Li, Curise Jia, Peng Jin, Zesen Cheng, Kehan Li, Jialu Sui, Chang Liu, Li Yuan  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Image Retrieval aims to retrieve corresponding images based on a given query. In application scenarios, users intend to express their retrieval intent through various query styles. However, current retrieval tasks predominantly focus on text-query retrieval exploration, leading to limited retrieval query options and potential ambiguity or bias in user intention. In this paper, we propose the Style-Diversified Query-Based Image Retrieval task, which enables retrieval based on various query styles. To facilitate the novel setting, we propose the first Diverse-Style Retrieval dataset, encompassing diverse query styles including text, sketch, low-resolution, and art. We also propose a light-weighted style-diversified retrieval framework. For various query style inputs, we apply the Gram Matrix to extract the query's textural features and cluster them into a style space with style-specific bases. Then we employ the style-init prompt tuning module to enable the visual encoder to comprehend the texture and style information of the query. Experiments demonstrate that our model, employing the style-init prompt tuning strategy, outperforms existing retrieval models on the style-diversified retrieval task. Moreover, style-diversified queries~(sketch+text, art+text, etc) can be simultaneously retrieved in our model. The auxiliary information from other queries enhances the retrieval performance within the respective query.  
  </ol>  
</details>  
**comments**: 16 pages, 7 figures  
  
### [Implicit Learning of Scene Geometry from Poses for Global Localization](http://arxiv.org/abs/2312.02029)  
Mohammad Altillawi, Shile Li, Sai Manoj Prakhya, Ziyuan Liu, Joan Serrat  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Global visual localization estimates the absolute pose of a camera using a single image, in a previously mapped area. Obtaining the pose from a single image enables many robotics and augmented/virtual reality applications. Inspired by latest advances in deep learning, many existing approaches directly learn and regress 6 DoF pose from an input image. However, these methods do not fully utilize the underlying scene geometry for pose regression. The challenge in monocular relocalization is the minimal availability of supervised training data, which is just the corresponding 6 DoF poses of the images. In this paper, we propose to utilize these minimal available labels (.i.e, poses) to learn the underlying 3D geometry of the scene and use the geometry to estimate the 6 DoF camera pose. We present a learning method that uses these pose labels and rigid alignment to learn two 3D geometric representations (\textit{X, Y, Z coordinates}) of the scene, one in camera coordinate frame and the other in global coordinate frame. Given a single image, it estimates these two 3D scene representations, which are then aligned to estimate a pose that matches the pose label. This formulation allows for the active inclusion of additional learning constraints to minimize 3D alignment errors between the two 3D scene representations, and 2D re-projection errors between the 3D global scene representation and 2D image pixels, resulting in improved localization accuracy. During inference, our model estimates the 3D scene geometry in camera and global frames and aligns them rigidly to obtain pose in real-time. We evaluate our work on three common visual localization datasets, conduct ablation studies, and show that our method exceeds state-of-the-art regression methods' pose accuracy on all datasets.  
  </ol>  
</details>  
**comments**: IEEE ROBOTICS AND AUTOMATION LETTERS. ACCEPTED NOVEMBER, 2023  
  
### [Language-only Efficient Training of Zero-shot Composed Image Retrieval](http://arxiv.org/abs/2312.01998)  
[[code](https://github.com/navervision/lincir)]  
Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, Sangdoo Yun  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, we let the new and original texts have the same latent embedding vector. With this simple strategy, LinCIR is surprisingly efficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in 48 minutes and shows the best ZS-CIR performances on four different CIR benchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised method on FashionIQ. Code is available at https://github.com/navervision/lincir  
  </ol>  
</details>  
**comments**: First two authors contributed equally; 16 pages, 2.9MB  
  
### [G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training](http://arxiv.org/abs/2312.01522)  
Che Liu, Cheng Ouyang, Sibo Cheng, Anand Shah, Wenjia Bai, Rossella Arcucci  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Recently, medical vision-language pre-training (VLP) has reached substantial progress to learn global visual representation from medical images and their paired radiology reports. However, medical imaging tasks in real world usually require finer granularity in visual features. These tasks include visual localization tasks (e.g., semantic segmentation, object detection) and visual grounding task. Yet, current medical VLP methods face challenges in learning these fine-grained features, as they primarily focus on brute-force alignment between image patches and individual text tokens for local visual feature learning, which is suboptimal for downstream dense prediction tasks. In this work, we propose a new VLP framework, named \textbf{G}lobal to \textbf{D}ense level representation learning (G2D) that achieves significantly improved granularity and more accurate grounding for the learned features, compared to existing medical VLP approaches. In particular, G2D learns dense and semantically-grounded image representations via a pseudo segmentation task parallel with the global vision-language alignment. Notably, generating pseudo segmentation targets does not incur extra trainable parameters: they are obtained on the fly during VLP with a parameter-free processor. G2D achieves superior performance across 6 medical imaging tasks and 25 diseases, particularly in semantic segmentation, which necessitates fine-grained, semantically-grounded image features. In this task, G2D surpasses peer models even when fine-tuned with just 1\% of the training data, compared to the 100\% used by these models. The code will be released upon acceptance.  
  </ol>  
</details>  
  
### [Improve Supervised Representation Learning with Masked Image Modeling](http://arxiv.org/abs/2312.00950)  
Kaifeng Chen, Daniel Salz, Huiwen Chang, Kihyuk Sohn, Dilip Krishnan, Mojtaba Seyedhosseini  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Training visual embeddings with labeled data supervision has been the de facto setup for representation learning in computer vision. Inspired by recent success of adopting masked image modeling (MIM) in self-supervised representation learning, we propose a simple yet effective setup that can easily integrate MIM into existing supervised training paradigms. In our design, in addition to the original classification task applied to a vision transformer image encoder, we add a shallow transformer-based decoder on top of the encoder and introduce an MIM task which tries to reconstruct image tokens based on masked image inputs. We show with minimal change in architecture and no overhead in inference that this setup is able to improve the quality of the learned representations for downstream tasks such as classification, image retrieval, and semantic segmentation. We conduct a comprehensive study and evaluation of our setup on public benchmarks. On ImageNet-1k, our ViT-B/14 model achieves 81.72% validation accuracy, 2.01% higher than the baseline model. On K-Nearest-Neighbor image retrieval evaluation with ImageNet-1k, the same model outperforms the baseline by 1.32%. We also show that this setup can be easily scaled to larger models and datasets. Code and checkpoints will be released.  
  </ol>  
</details>  
  
### [Grounding Everything: Emerging Localization Properties in Vision-Language Transformers](http://arxiv.org/abs/2312.00878)  
[[code](https://github.com/walbouss/gem)]  
Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Vision-language foundation models have shown remarkable performance in various zero-shot settings such as image retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot localization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object localization without any fine-tuning. To leverage those capabilities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery to a self-self attention path. We show that the concept of self-self attention corresponds to clustering, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various benchmark tasks and datasets for semantic segmentation. It shows that GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art results on the recently proposed OpenImagesV7 large-scale segmentation benchmark.  
  </ol>  
</details>  
**comments**: Code available at https://github.com/WalBouss/GEM  
  
### [Global Localization: Utilizing Relative Spatio-Temporal Geometric Constraints from Adjacent and Distant Cameras](http://arxiv.org/abs/2312.00500)  
Mohammad Altillawi, Zador Pataki, Shile Li, Ziyuan Liu  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Re-localizing a camera from a single image in a previously mapped area is vital for many computer vision applications in robotics and augmented/virtual reality. In this work, we address the problem of estimating the 6 DoF camera pose relative to a global frame from a single image. We propose to leverage a novel network of relative spatial and temporal geometric constraints to guide the training of a Deep Network for localization. We employ simultaneously spatial and temporal relative pose constraints that are obtained not only from adjacent camera frames but also from camera frames that are distant in the spatio-temporal space of the scene. We show that our method, through these constraints, is capable of learning to localize when little or very sparse ground-truth 3D coordinates are available. In our experiments, this is less than 1% of available ground-truth data. We evaluate our method on 3 common visual localization datasets and show that it outperforms other direct pose estimation methods.  
  </ol>  
</details>  
**comments**: To be published in the proceedings of IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) 2023  
  
### [HKUST at SemEval-2023 Task 1: Visual Word Sense Disambiguation with Context Augmentation and Visual Assistance](http://arxiv.org/abs/2311.18273)  
[[code](https://github.com/thomas-yin/semeval-2023-task1)]  
Zhuohao Yin, Xin Huang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Visual Word Sense Disambiguation (VWSD) is a multi-modal task that aims to select, among a batch of candidate images, the one that best entails the target word's meaning within a limited context. In this paper, we propose a multi-modal retrieval framework that maximally leverages pretrained Vision-Language models, as well as open knowledge bases and datasets. Our system consists of the following key components: (1) Gloss matching: a pretrained bi-encoder model is used to match contexts with proper senses of the target words; (2) Prompting: matched glosses and other textual information, such as synonyms, are incorporated using a prompting template; (3) Image retrieval: semantically matching images are retrieved from large open datasets using prompts as queries; (4) Modality fusion: contextual information from different modalities are fused and used for prediction. Although our system does not produce the most competitive results at SemEval-2023 Task 1, we are still able to beat nearly half of the teams. More importantly, our experiments reveal acute insights for the field of Word Sense Disambiguation (WSD) and multi-modal learning. Our code is available on GitHub.  
  </ol>  
</details>  
  
### [Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models](http://arxiv.org/abs/2311.18237)  
Raviteja Vemulapalli, Hadi Pouransari, Fartash Faghri, Sachin Mehta, Mehrdad Farajtabar, Mohammad Rastegari, Oncel Tuzel  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high memory and compute requirements, these models cannot be deployed in resource constrained settings. This raises an important question: How can we utilize the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data? In this work, we answer this question by proposing a simple and highly effective task-oriented knowledge transfer approach to leverage pretrained VFMs for effective training of small task-specific models. Our experimental results on four target tasks under limited labeled data settings show that the proposed knowledge transfer approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining and supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and propose an image retrieval-based approach for curating effective transfer sets.  
  </ol>  
</details>  
  
  



## Keypoint Detection  

### [Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version)](http://arxiv.org/abs/2312.00592)  
Emma Cramer, Jonas Reiher, Sebastian Trimpe  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Reinforcement learning (RL) for robot control typically requires a detailed representation of the environment state, including information about task-relevant objects not directly measurable. Keypoint detectors, such as spatial autoencoders (SAEs), are a common approach to extracting a low-dimensional representation from high-dimensional image data. SAEs aim at spatial features such as object positions, which are often useful representations in robotic RL. However, whether an SAE is actually able to track objects in the scene and thus yields a spatial state representation well suited for RL tasks has rarely been examined due to a lack of established metrics. In this paper, we propose to assess the performance of an SAE instance by measuring how well keypoints track ground truth objects in images. We present a computationally lightweight metric and use it to evaluate common baseline SAE architectures on image data from a simulated robot task. We find that common SAEs differ substantially in their spatial extraction capability. Furthermore, we validate that SAEs that perform well in our metric achieve superior performance when used in downstream RL. Thus, our metric is an effective and lightweight indicator of RL performance before executing expensive RL training. Building on these insights, we identify three key modifications of SAE architectures to improve tracking performance. We make our code available at anonymous.4open.science/r/sae-rl.  
  </ol>  
</details>  
  
### [Utilizing Radiomic Feature Analysis For Automated MRI Keypoint Detection: Enhancing Graph Applications](http://arxiv.org/abs/2311.18281)  
Sahar Almahfouz Nasser, Shashwat Pathak, Keshav Singhal, Mohit Meena, Nihar Gupte, Ananya Chinmaya, Prateek Garg, Amit Sethi  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Graph neural networks (GNNs) present a promising alternative to CNNs and transformers in certain image processing applications due to their parameter-efficiency in modeling spatial relationships. Currently, a major area of research involves the converting non-graph input data for GNN-based models, notably in scenarios where the data originates from images. One approach involves converting images into nodes by identifying significant keypoints within them. Super-Retina, a semi-supervised technique, has been utilized for detecting keypoints in retinal images. However, its limitations lie in the dependency on a small initial set of ground truth keypoints, which is progressively expanded to detect more keypoints. Having encountered difficulties in detecting consistent initial keypoints in brain images using SIFT and LoFTR, we proposed a new approach: radiomic feature-based keypoint detection. Demonstrating the anatomical significance of the detected keypoints was achieved by showcasing their efficacy in improving registration processes guided by these keypoints. Subsequently, these keypoints were employed as the ground truth for the keypoint detection method (LK-SuperRetina). Furthermore, the study showcases the application of GNNs in image matching, highlighting their superior performance in terms of both the number of good matches and confidence scores. This research sets the stage for expanding GNN applications into various other applications, including but not limited to image classification, segmentation, and registration.  
  </ol>  
</details>  
  
### [Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features](http://arxiv.org/abs/2311.18113)  
Thomas Wimmer, Peter Wonka, Maks Ovsjanikov  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    With the immense growth of dataset sizes and computing resources in recent years, so-called foundation models have become popular in NLP and vision tasks. In this work, we propose to explore foundation models for the task of keypoint detection on 3D shapes. A unique characteristic of keypoint detection is that it requires semantic and geometric awareness while demanding high localization accuracy. To address this problem, we propose, first, to back-project features from large pre-trained 2D vision models onto 3D shapes and employ them for this task. We show that we obtain robust 3D features that contain rich semantic information and analyze multiple candidate features stemming from different 2D foundation models. Second, we employ a keypoint candidate optimization module which aims to match the average observed distribution of keypoints on the shape and is guided by the back-projected features. The resulting approach achieves a new state of the art for few-shot keypoint detection on the KeyPointNet dataset, almost doubling the performance of the previous best methods.  
  </ol>  
</details>  
**comments**: Project page: https://wimmerth.github.io/back-to-3d.html  
  
### [Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features](http://arxiv.org/abs/2311.17024)  
Niladri Shekhar Dutt, Sanjeev Muralikrishnan, Niloy J. Mitra  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We present Diff3F as a simple, robust, and class-agnostic feature descriptor that can be computed for untextured input shapes (meshes or point clouds). Our method distills diffusion features from image foundational models onto input shapes. Specifically, we use the input shapes to produce depth and normal maps as guidance for conditional image synthesis, and in the process produce (diffusion) features in 2D that we subsequently lift and aggregate on the original surface. Our key observation is that even if the conditional image generations obtained from multi-view rendering of the input shapes are inconsistent, the associated image features are robust and can be directly aggregated across views. This produces semantic features on the input shapes, without requiring additional data or training. We perform extensive experiments on multiple benchmarks (SHREC'19, SHREC'20, and TOSCA) and demonstrate that our features, being semantic instead of geometric, produce reliable correspondence across both isometeric and non-isometrically related shape families.  
  </ol>  
</details>  
  
### [Riemannian Self-Attention Mechanism for SPD Networks](http://arxiv.org/abs/2311.16738)  
Rui Wang, Xiao-Jun Wu, Hui Li, Josef Kittler  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Symmetric positive definite (SPD) matrix has been demonstrated to be an effective feature descriptor in many scientific areas, as it can encode spatiotemporal statistics of the data adequately on a curved Riemannian manifold, i.e., SPD manifold. Although there are many different ways to design network architectures for SPD matrix nonlinear learning, very few solutions explicitly mine the geometrical dependencies of features at different layers. Motivated by the great success of self-attention mechanism in capturing long-range relationships, an SPD manifold self-attention mechanism (SMSA) is proposed in this paper using some manifold-valued geometric operations, mainly the Riemannian metric, Riemannian mean, and Riemannian optimization. Then, an SMSA-based geometric learning module (SMSA-GLM) is designed for the sake of improving the discrimination of the generated deep structured representations. Extensive experimental results achieved on three benchmarking datasets show that our modification against the baseline network further alleviates the information degradation problem and leads to improved accuracy.  
  </ol>  
</details>  
**comments**: 14 pages, 10 figures, 5 tables  
  
### [A manometric feature descriptor with linear-SVM to distinguish esophageal contraction vigor](http://arxiv.org/abs/2311.15609)  
Jialin Liu, Lu Yan, Xiaowei Liu, Yuzhuo Dai, Fanggen Lu, Yuanting Ma, Muzhou Hou, Zheng Wang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    n clinical, if a patient presents with nonmechanical obstructive dysphagia, esophageal chest pain, and gastro esophageal reflux symptoms, the physician will usually assess the esophageal dynamic function. High-resolution manometry (HRM) is a clinically commonly used technique for detection of esophageal dynamic function comprehensively and objectively. However, after the results of HRM are obtained, doctors still need to evaluate by a variety of parameters. This work is burdensome, and the process is complex. We conducted image processing of HRM to predict the esophageal contraction vigor for assisting the evaluation of esophageal dynamic function. Firstly, we used Feature-Extraction and Histogram of Gradients (FE-HOG) to analyses feature of proposal of swallow (PoS) to further extract higher-order features. Then we determine the classification of esophageal contraction vigor normal, weak and failed by using linear-SVM according to these features. Our data set includes 3000 training sets, 500 validation sets and 411 test sets. After verification our accuracy reaches 86.83%, which is higher than other common machine learning methods.  
  </ol>  
</details>  
  
### [Instance-aware 3D Semantic Segmentation powered by Shape Generators and Classifiers](http://arxiv.org/abs/2311.12291)  
Bo Sun, Qixing Huang, Xiangru Huang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Existing 3D semantic segmentation methods rely on point-wise or voxel-wise feature descriptors to output segmentation predictions. However, these descriptors are often supervised at point or voxel level, leading to segmentation models that can behave poorly at instance-level. In this paper, we proposed a novel instance-aware approach for 3D semantic segmentation. Our method combines several geometry processing tasks supervised at instance-level to promote the consistency of the learned feature representation. Specifically, our methods use shape generators and shape classifiers to perform shape reconstruction and classification tasks for each shape instance. This enforces the feature representation to faithfully encode both structural and local shape information, with an awareness of shape instances. In the experiments, our method significantly outperform existing approaches in 3D semantic segmentation on several public benchmarks, such as Waymo Open Dataset, SemanticKITTI and ScanNetV2.  
  </ol>  
</details>  
  
### [CurriculumLoc: Enhancing Cross-Domain Geolocalization through Multi-Stage Refinement](http://arxiv.org/abs/2311.11604)  
[[code](https://github.com/npupilab/curriculumloc)]  
Boni Hu, Lin Chen, Runjian Chen, Shuhui Bu, Pengcheng Han, Haowei Li  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Visual geolocalization is a cost-effective and scalable task that involves matching one or more query images, taken at some unknown location, to a set of geo-tagged reference images. Existing methods, devoted to semantic features representation, evolving towards robustness to a wide variety between query and reference, including illumination and viewpoint changes, as well as scale and seasonal variations. However, practical visual geolocalization approaches need to be robust in appearance changing and extreme viewpoint variation conditions, while providing accurate global location estimates. Therefore, inspired by curriculum design, human learn general knowledge first and then delve into professional expertise. We first recognize semantic scene and then measure geometric structure. Our approach, termed CurriculumLoc, involves a delicate design of multi-stage refinement pipeline and a novel keypoint detection and description with global semantic awareness and local geometric verification. We rerank candidates and solve a particular cross-domain perspective-n-point (PnP) problem based on these keypoints and corresponding descriptors, position refinement occurs incrementally. The extensive experimental results on our collected dataset, TerraTrack and a benchmark dataset, ALTO, demonstrate that our approach results in the aforementioned desirable characteristics of a practical visual geolocalization solution. Additionally, we achieve new high recall@1 scores of 62.6% and 94.5% on ALTO, with two different distances metrics, respectively. Dataset, code and trained models are publicly available on https://github.com/npupilab/CurriculumLoc.  
  </ol>  
</details>  
**comments**: 14 pages, 15 figures  
  
### [Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration](http://arxiv.org/abs/2311.10361)  
Paul J. Claasen, J. P. de Villiers  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    A novel Bayesian framework is proposed, which explicitly relates the homography of one video frame to the next through an affine transformation while explicitly modelling keypoint uncertainty. The literature has previously used differential homography between subsequent frames, but not in a Bayesian setting. In cases where Bayesian methods have been applied, camera motion is not adequately modelled, and keypoints are treated as deterministic. The proposed method, Bayesian Homography Inference from Tracked Keypoints (BHITK), employs a two-stage Kalman filter and significantly improves existing methods. Existing keypoint detection methods may be easily augmented with BHITK. It enables less sophisticated and less computationally expensive methods to outperform the state-of-the-art approaches in most homography evaluation metrics. Furthermore, the homography annotations of the WorldCup and TS-WorldCup datasets have been refined using a custom homography annotation tool released for public use. The refined datasets are consolidated and released as the consolidated and refined WorldCup (CARWC) dataset.  
  </ol>  
</details>  
**comments**: Submitted to Expert Systems with Applications and currently under
  review  
  
### [Processing and Segmentation of Human Teeth from 2D Images using Weakly Supervised Learning](http://arxiv.org/abs/2311.07398)  
Tomáš Kunzo, Viktor Kocur, Lukáš Gajdošech, Martin Madaras  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Teeth segmentation is an essential task in dental image analysis for accurate diagnosis and treatment planning. While supervised deep learning methods can be utilized for teeth segmentation, they often require extensive manual annotation of segmentation masks, which is time-consuming and costly. In this research, we propose a weakly supervised approach for teeth segmentation that reduces the need for manual annotation. Our method utilizes the output heatmaps and intermediate feature maps from a keypoint detection network to guide the segmentation process. We introduce the TriDental dataset, consisting of 3000 oral cavity images annotated with teeth keypoints, to train a teeth keypoint detection network. We combine feature maps from different layers of the keypoint detection network, enabling accurate teeth segmentation without explicit segmentation annotations. The detected keypoints are also used for further refinement of the segmentation masks. Experimental results on the TriDental dataset demonstrate the superiority of our approach in terms of accuracy and robustness compared to state-of-the-art segmentation methods. Our method offers a cost-effective and efficient solution for teeth segmentation in real-world dental applications, eliminating the need for extensive manual annotation efforts.  
  </ol>  
</details>  
**comments**: Pubslished in 2023 World Symposium on Digital Intelligence for
  Systems and Machines (DISA) Proceedings. Published version copyrighted by
  IEEE, pre-print released in accordance with the copyright agreement  
  
  



## Image Matching  

### [Visual Geometry Grounded Deep Structure From Motion](http://arxiv.org/abs/2312.04563)  
Jianyuan Wang, Nikita Karaev, Christian Rupprecht, David Novotny  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.  
  </ol>  
</details>  
**comments**: 8 figures. Project page: https://vggsfm.github.io/  
  
### [Steerers: A framework for rotation equivariant keypoint descriptors](http://arxiv.org/abs/2312.02152)  
[[code](https://github.com/georg-bn/rotation-steerers)]  
Georg Bökman, Johan Edstedt, Michael Felsberg, Fredrik Kahl  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. We instead learn a linear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated. From representation theory we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in all of these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360. We publish code and model weights at github.com/georg-bn/rotation-steerers.  
  </ol>  
</details>  
  
### [DSeg: Direct Line Segments Detection](http://arxiv.org/abs/2311.18344)  
Berger Cyrille, Lacroix Simon  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    This paper presents a model-driven approach to detect image line segments. The approach incrementally detects segments on the gradient image using a linear Kalman filter that estimates the supporting line parameters and their associated variances. The algorithm is fast and robust with respect to image noise and illumination variations, it allows the detection of longer line segments than data-driven approaches, and does not require any tedious parameters tuning. An extension of the algorithm that exploits a pyramidal approach to enhance the quality of results is proposed. Results with varying scene illumination and comparisons to classic existing approaches are presented.  
  </ol>  
</details>  
  
### [Utilizing Radiomic Feature Analysis For Automated MRI Keypoint Detection: Enhancing Graph Applications](http://arxiv.org/abs/2311.18281)  
Sahar Almahfouz Nasser, Shashwat Pathak, Keshav Singhal, Mohit Meena, Nihar Gupte, Ananya Chinmaya, Prateek Garg, Amit Sethi  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Graph neural networks (GNNs) present a promising alternative to CNNs and transformers in certain image processing applications due to their parameter-efficiency in modeling spatial relationships. Currently, a major area of research involves the converting non-graph input data for GNN-based models, notably in scenarios where the data originates from images. One approach involves converting images into nodes by identifying significant keypoints within them. Super-Retina, a semi-supervised technique, has been utilized for detecting keypoints in retinal images. However, its limitations lie in the dependency on a small initial set of ground truth keypoints, which is progressively expanded to detect more keypoints. Having encountered difficulties in detecting consistent initial keypoints in brain images using SIFT and LoFTR, we proposed a new approach: radiomic feature-based keypoint detection. Demonstrating the anatomical significance of the detected keypoints was achieved by showcasing their efficacy in improving registration processes guided by these keypoints. Subsequently, these keypoints were employed as the ground truth for the keypoint detection method (LK-SuperRetina). Furthermore, the study showcases the application of GNNs in image matching, highlighting their superior performance in terms of both the number of good matches and confidence scores. This research sets the stage for expanding GNN applications into various other applications, including but not limited to image classification, segmentation, and registration.  
  </ol>  
</details>  
  
### [LGFCTR: Local and Global Feature Convolutional Transformer for Image Matching](http://arxiv.org/abs/2311.17571)  
Wenhao Zhong, Jie Jiang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Image matching that finding robust and accurate correspondences across images is a challenging task under extreme conditions. Capturing local and global features simultaneously is an important way to mitigate such an issue but recent transformer-based decoders were still stuck in the issues that CNN-based encoders only extract local features and the transformers lack locality. Inspired by the locality and implicit positional encoding of convolutions, a novel convolutional transformer is proposed to capture both local contexts and global structures more sufficiently for detector-free matching. Firstly, a universal FPN-like framework captures global structures in self-encoder as well as cross-decoder by transformers and compensates local contexts as well as implicit positional encoding by convolutions. Secondly, a novel convolutional transformer module explores multi-scale long range dependencies by a novel multi-scale attention and further aggregates local information inside dependencies for enhancing locality. Finally, a novel regression-based sub-pixel refinement module exploits the whole fine-grained window features for fine-level positional deviation regression. The proposed method achieves superior performances on a wide range of benchmarks. The code will be available on https://github.com/zwh0527/LGFCTR.  
  </ol>  
</details>  
**comments**: 8 pages of main text, 7 pages of supplementary material, 3 pages of
  references, 6 figures in main text and 8 figures in supplementary material, 5
  tables in main text and 2 tables in supplementary material  
  
### [Zero-shot Translation of Attention Patterns in VQA Models to Natural Language](http://arxiv.org/abs/2311.05043)  
[[code](https://github.com/explainableml/zs-a2t)]  
Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Converting a model's internals to text can yield human-understandable insights about the model. Inspired by the recent success of training-free approaches for image captioning, we propose ZS-A2T, a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer, as inputs. The LLM is guided to select tokens which describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g. attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, giving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available at: https://github.com/ExplainableML/ZS-A2T.  
  </ol>  
</details>  
**comments**: Published in GCPR 2023  
  
### [An invariant feature extraction for multi-modal images matching](http://arxiv.org/abs/2311.02842)  
Chenzhong Gao, Wei Li  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    This paper aims at providing an effective multi-modal images invariant feature extraction and matching algorithm for the application of multi-source data analysis. Focusing on the differences and correlation of multi-modal images, a feature-based matching algorithm is implemented. The key technologies include phase congruency (PC) and Shi-Tomasi feature point for keypoints detection, LogGabor filter and a weighted partial main orientation map (WPMOM) for feature extraction, and a multi-scale process to deal with scale differences and optimize matching results. The experimental results on practical data from multiple sources prove that the algorithm has effective performances on multi-modal images, which achieves accurate spatial alignment, showing practical application value and good generalization.  
  </ol>  
</details>  
  
### [RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments](http://arxiv.org/abs/2310.15072)  
Jinyu Li, Xiaokun Pan, Gan Huang, Ziyang Zhang, Nan Wang, Hujun Bao, Guofeng Zhang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    It is typically challenging for visual or visual-inertial odometry systems to handle the problems of dynamic scenes and pure rotation. In this work, we design a novel visual-inertial odometry (VIO) system called RD-VIO to handle both of these two problems. Firstly, we propose an IMU-PARSAC algorithm which can robustly detect and match keypoints in a two-stage process. In the first state, landmarks are matched with new keypoints using visual and IMU measurements. We collect statistical information from the matching and then guide the intra-keypoint matching in the second stage. Secondly, to handle the problem of pure rotation, we detect the motion type and adapt the deferred-triangulation technique during the data-association process. We make the pure-rotational frames into the special subframes. When solving the visual-inertial bundle adjustment, they provide additional constraints to the pure-rotational motion. We evaluate the proposed VIO system on public datasets. Experiments show the proposed RD-VIO has obvious advantages over other methods in dynamic environments.  
  </ol>  
</details>  
  
### [Player Re-Identification Using Body Part Appearences](http://arxiv.org/abs/2310.14469)  
Mahesh Bhosale, Abhishek Kumar, David Doermann  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We propose a neural network architecture that learns body part appearances for soccer player re-identification. Our model consists of a two-stream network (one stream for appearance map extraction and the other for body part map extraction) and a bilinear-pooling layer that generates and spatially pools the body part map. Each local feature of the body part map is obtained by a bilinear mapping of the corresponding local appearance and body part descriptors. Our novel representation yields a robust image-matching feature map, which results from combining the local similarities of the relevant body parts with the weighted appearance similarity. Our model does not require any part annotation on the SoccerNet-V3 re-identification dataset to train the network. Instead, we use a sub-network of an existing pose estimation network (OpenPose) to initialize the part substream and then train the entire network to minimize the triplet loss. The appearance stream is pre-trained on the ImageNet dataset, and the part stream is trained from scratch for the SoccerNet-V3 dataset. We demonstrate the validity of our model by showing that it outperforms state-of-the-art models such as OsNet and InceptionNet.  
  </ol>  
</details>  
  
### [FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer](http://arxiv.org/abs/2310.13605)  
Xinyu Zhang, Li Wang, Zhiqiang Jiang, Kun Dai, Tao Xie, Lei Yang, Wenhao Yu, Yang Shen, Jun Li  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.  
  </ol>  
</details>  
  
  



## NeRF  

### [MuRF: Multi-Baseline Radiance Fields](http://arxiv.org/abs/2312.04565)  
[[code](https://github.com/autonomousvision/murf)]  
Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, Fisher Yu  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward approach to solving sparse view synthesis under multiple different baseline settings (small and large baselines, and different number of input views). To render a target novel view, we discretize the 3D space into planes parallel to the target image plane, and accordingly construct a target view frustum volume. Such a target volume representation is spatially aligned with the target view, which effectively aggregates relevant information from the input views for high-quality rendering. It also facilitates subsequent radiance field regression with a convolutional network thanks to its axis-aligned nature. The 3D context modeled by the convolutional network enables our method to synthesis sharper scene structures than prior works. Our MuRF achieves state-of-the-art performance across multiple different baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K and LLFF). We also show promising zero-shot generalization abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability of MuRF.  
  </ol>  
</details>  
**comments**: Project page: https://haofeixu.github.io/murf/  
  
### [EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS](http://arxiv.org/abs/2312.04564)  
Sharath Girish, Kamal Gupta, Abhinav Shrivastava  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach results in scene representations with fewer Gaussians and quantized representations, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce memory by more than an order of magnitude all while maintaining the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x less memory and faster training/inference speed. Project page and code is available https://efficientgaussian.github.io  
  </ol>  
</details>  
**comments**: Website: https://efficientgaussian.github.io Code:
  https://github.com/Sharath-girish/efficientgaussian  
  
### [Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection](http://arxiv.org/abs/2312.04527)  
Kohei Yamashita, Vincent Lepetit, Ko Nishino  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Computer vision has long relied on two kinds of correspondences: pixel correspondences in images and 3D correspondences on object surfaces. Is there another kind, and if there is, what can they do for us? In this paper, we introduce correspondences of the third kind we call reflection correspondences and show that they can help estimate camera pose by just looking at objects without relying on the background. Reflection correspondences are point correspondences in the reflected world, i.e., the scene reflected by the object surface. The object geometry and reflectance alters the scene geometrically and radiometrically, respectively, causing incorrect pixel correspondences. Geometry recovered from each image is also hampered by distortions, namely generalized bas-relief ambiguity, leading to erroneous 3D correspondences. We show that reflection correspondences can resolve the ambiguities arising from these distortions. We introduce a neural correspondence estimator and a RANSAC algorithm that fully leverages all three kinds of correspondences for robust and accurate joint camera pose and object shape estimation just from the object appearance. The method expands the horizon of numerous downstream tasks, including camera pose estimation for appearance modeling (e.g., NeRF) and motion estimation of reflective objects (e.g., cars on the road), to name a few, as it relieves the requirement of overlapping background.  
  </ol>  
</details>  
  
### [Multi-View Unsupervised Image Generation with Cross Attention Guidance](http://arxiv.org/abs/2312.04337)  
Llukman Cerkezi, Aram Davtyan, Sepehr Sameni, Paolo Favaro  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    The growing interest in novel view synthesis, driven by Neural Radiance Field (NeRF) models, is hindered by scalability issues due to their reliance on precisely annotated multi-view images. Recent models address this by fine-tuning large text2image diffusion models on synthetic multi-view data. Despite robust zero-shot generalization, they may need post-processing and can face quality issues due to the synthetic-real domain gap. This paper introduces a novel pipeline for unsupervised training of a pose-conditioned diffusion model on single-category datasets. With the help of pretrained self-supervised Vision Transformers (DINOv2), we identify object poses by clustering the dataset through comparing visibility and locations of specific object parts. The pose-conditioned diffusion model, trained on pose labels, and equipped with cross-frame attention at inference time ensures cross-view consistency, that is further aided by our novel hard-attention guidance. Our model, MIRAGE, surpasses prior work in novel view synthesis on real images. Furthermore, MIRAGE is robust to diverse textures and geometries, as demonstrated with our experiments on synthetic images generated with pretrained Stable Diffusion.  
  </ol>  
</details>  
  
### [Towards 4D Human Video Stylization](http://arxiv.org/abs/2312.04143)  
[[code](https://github.com/tiantianwang/4d_video_stylization)]  
Tiantian Wang, Xinxin Zuo, Fangzhou Mu, Jian Wang, Ming-Hsuan Yang  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We present a first step towards 4D (3D and time) human video stylization, which addresses style transfer, novel view synthesis and human animation within a unified framework. While numerous video stylization methods have been developed, they are often restricted to rendering images in specific viewpoints of the input video, lacking the capability to generalize to novel views and novel poses in dynamic scenes. To overcome these limitations, we leverage Neural Radiance Fields (NeRFs) to represent videos, conducting stylization in the rendered feature space. Our innovative approach involves the simultaneous representation of both the human subject and the surrounding scene using two NeRFs. This dual representation facilitates the animation of human subjects across various poses and novel viewpoints. Specifically, we introduce a novel geometry-guided tri-plane representation, significantly enhancing feature representation robustness compared to direct tri-plane optimization. Following the video reconstruction, stylization is performed within the NeRFs' rendered feature space. Extensive experiments demonstrate that the proposed method strikes a superior balance between stylized textures and temporal coherence, surpassing existing approaches. Furthermore, our framework uniquely extends its capabilities to accommodate novel poses and viewpoints, making it a versatile tool for creative human video stylization.  
  </ol>  
</details>  
**comments**: Under Review  
  
### [Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial Reconstruction](http://arxiv.org/abs/2312.04106)  
Jiayi Kong, Baixin Xu, Xurui Song, Chen Qian, Jun Luo, Ying He  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Neural radiance fields (NeRF) typically require a complete set of images taken from multiple camera perspectives to accurately reconstruct geometric details. However, this approach raise significant privacy concerns in the context of facial reconstruction. The critical need for privacy protection often leads invidividuals to be reluctant in sharing their facial images, due to fears of potential misuse or security risks. Addressing these concerns, we propose a method that leverages privacy-preserving images for reconstructing 3D head geometry within the NeRF framework. Our method stands apart from traditional facial reconstruction techniques as it does not depend on RGB information from images containing sensitive facial data. Instead, it effectively generates plausible facial geometry using a series of identity-obscured inputs, thereby protecting facial privacy.  
  </ol>  
</details>  
  
### [Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion](http://arxiv.org/abs/2312.03869)  
Kira Prabhu, Jane Wu, Lynn Tsai, Peter Hedman, Dan B Goldman, Ben Poole, Michael Broxton  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    This paper presents a novel approach to inpainting 3D regions of a scene, given masked multi-view images, by distilling a 2D diffusion model into a learned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods that explicitly condition the diffusion model on camera pose or multi-view information, our diffusion model is conditioned only on a single masked 2D image. Nevertheless, we show that this 2D diffusion model can still serve as a generative prior in a 3D multi-view reconstruction problem where we optimize a NeRF using a combination of score distillation sampling and NeRF reconstruction losses. Predicted depth is used as additional supervision to encourage accurate geometry. We compare our approach to 3D inpainting methods that focus on object removal. Because our method can generate content to fill any 3D masked region, we additionally demonstrate 3D object completion, 3D object replacement, and 3D scene completion.  
  </ol>  
</details>  
  
### [Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle](http://arxiv.org/abs/2312.03431)  
Youtian Lin, Zuozhuo Dai, Siyu Zhu, Yao Yao  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a $5\times$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow  
  </ol>  
</details>  
  
### [Artist-Friendly Relightable and Animatable Neural Heads](http://arxiv.org/abs/2312.03420)  
Yingyan Xu, Prashanth Chandran, Sebastian Weiss, Markus Gross, Gaspard Zoss, Derek Bradley  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    An increasingly common approach for creating photo-realistic digital avatars is through the use of volumetric neural fields. The original neural radiance field (NeRF) allowed for impressive novel view synthesis of static heads when trained on a set of multi-view images, and follow up methods showed that these neural representations can be extended to dynamic avatars. Recently, new variants also surpassed the usual drawback of baked-in illumination in neural representations, showing that static neural avatars can be relit in any environment. In this work we simultaneously tackle both the motion and illumination problem, proposing a new method for relightable and animatable neural heads. Our method builds on a proven dynamic avatar approach based on a mixture of volumetric primitives, combined with a recently-proposed lightweight hardware setup for relightable neural fields, and includes a novel architecture that allows relighting dynamic neural avatars performing unseen expressions in any environment, even with nearfield illumination and viewpoints.  
  </ol>  
</details>  
  
### [Evaluating the point cloud of individual trees generated from images based on Neural Radiance fields (NeRF) method](http://arxiv.org/abs/2312.03372)  
Hongyu Huang, Guoji Tian, Chongcheng Chen  
<details>  
  <summary>Abstract</summary>  
  <ol>  
    Three-dimensional (3D) reconstruction of trees has always been a key task in precision forestry management and research. Due to the complex branch morphological structure of trees themselves and the occlusions from tree stems, branches and foliage, it is difficult to recreate a complete three-dimensional tree model from a two-dimensional image by conventional photogrammetric methods. In this study, based on tree images collected by various cameras in different ways, the Neural Radiance Fields (NeRF) method was used for individual tree reconstruction and the exported point cloud models are compared with point cloud derived from photogrammetric reconstruction and laser scanning methods. The results show that the NeRF method performs well in individual tree 3D reconstruction, as it has higher successful reconstruction rate, better reconstruction in the canopy area, it requires less amount of images as input. Compared with photogrammetric reconstruction method, NeRF has significant advantages in reconstruction efficiency and is adaptable to complex scenes, but the generated point cloud tends to be noisy and low resolution. The accuracy of tree structural parameters (tree height and diameter at breast height) extracted from the photogrammetric point cloud is still higher than those of derived from the NeRF point cloud. The results of this study illustrate the great potential of NeRF method for individual tree reconstruction, and it provides new ideas and research directions for 3D reconstruction and visualization of complex forest scenes.  
  </ol>  
</details>  
**comments**: 25 pages; 6 figures  
  
  



