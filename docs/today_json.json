{"Image Matching": {"2408.04823": {"paper_title": "One Shot is Enough for Sequential Infrared Small Target Segmentation", "paper_abstract": "Infrared small target sequences exhibit strong similarities between frames and contain rich contextual information, which motivates us to achieve sequential infrared small target segmentation with minimal data. Inspired by the success of large segmentation models led by Segment Anything Model (SAM) across various downstream tasks, we propose a one-shot and training-free method that perfectly adapts SAM's zero-shot generalization capabilities to sequential infrared small target segmentation. Given one annotated frame as a reference, our method can accurately segment small targets in other frames of the sequence. Specifically, we first obtain a confidence map through local feature matching between reference image and test image. Then, the highest point in the confidence map is as a prompt, and we design the Point Prompt-Centric Focusing (PPCF) module to address the over-segmentation of small targets with blurry boundaries. Subsequently, to prevent miss and false detections, we introduce the Triple-Level Ensemble (TLE) module that ensembles the masks obtained at different levels from the first two steps to produce the final mask. Experiments demonstrate that our method requires only one shot to achieve comparable performance to state-of-the-art methods based on traditional many-shot supervision and even superior performance in a few-shot setting. Moreover, ablation studies confirm the robustness of our approach to variations in one-shot samples, changes in scenes, and the presence of multiple targets.", "paper_authors": "Bingbing Dan, Meihui Li, Tao Tang, Jing Zhang", "update_time": "2024-08-09", "comments": null, "paper_url": "http://arxiv.org/abs/2408.04823", "paper_id": "2408.04823", "code_url": null}}, "NeRF": {"2408.05008": {"paper_title": "DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified Flow", "paper_abstract": "The Score Distillation Sampling (SDS), which exploits pretrained text-to-image model diffusion models as priors to 3D model training, has achieved significant success. Currently, the flow-based diffusion model has become a new trend for generations. Yet, adapting SDS to flow-based diffusion models in 3D generation remains unexplored. Our work is aimed to bridge this gap. In this paper, we adapt SDS to rectified flow and re-examine the over-smoothing issue under this novel framework. The issue can be explained that the model learns an average of multiple ODE trajectories. Then we propose DreamCouple, which instead of randomly sampling noise, uses a rectified flow model to find the coupled noise. Its Unique Couple Matching (UCM) loss guides the model to learn different trajectories and thus solves the over-smoothing issue. We apply our method to both NeRF and 3D Gaussian splatting and achieve state-of-the-art performances. We also identify some other interesting open questions such as initialization issues for NeRF and faster training convergence. Our code will be released soon.", "paper_authors": "Hangyu Li, Xiangxiang Chu, Dingyuan Shi", "update_time": "2024-08-09", "comments": "Tech Report", "paper_url": "http://arxiv.org/abs/2408.05008", "paper_id": "2408.05008", "code_url": null}, "2408.04803": {"paper_title": "FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid Scene-Specific Adaptation", "paper_abstract": "In this paper, we address the challenge of generating novel views of real-world objects with limited multi-view images through our proposed approach, FewShotNeRF. Our method utilizes meta-learning to acquire optimal initialization, facilitating rapid adaptation of a Neural Radiance Field (NeRF) to specific scenes. The focus of our meta-learning process is on capturing shared geometry and textures within a category, embedded in the weight initialization. This approach expedites the learning process of NeRFs and leverages recent advancements in positional encodings to reduce the time required for fitting a NeRF to a scene, thereby accelerating the inner loop optimization of meta-learning. Notably, our method enables meta-learning on a large number of 3D scenes to establish a robust 3D prior for various categories. Through extensive evaluations on the Common Objects in 3D open source dataset, we empirically demonstrate the efficacy and potential of meta-learning in generating high-quality novel views of objects.", "paper_authors": "Piraveen Sivakumar, Paul Janson, Jathushan Rajasegaran, Thanuja Ambegoda", "update_time": "2024-08-09", "comments": null, "paper_url": "http://arxiv.org/abs/2408.04803", "paper_id": "2408.04803", "code_url": null}}}