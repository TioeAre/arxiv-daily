{"SLAM": {"2502.13708": {"paper_title": "Active Illumination for Visual Ego-Motion Estimation in the Dark", "paper_abstract": "Visual Odometry (VO) and Visual SLAM (V-SLAM) systems often struggle in low-light and dark environments due to the lack of robust visual features. In this paper, we propose a novel active illumination framework to enhance the performance of VO and V-SLAM algorithms in these challenging conditions. The developed approach dynamically controls a moving light source to illuminate highly textured areas, thereby improving feature extraction and tracking. Specifically, a detector block, which incorporates a deep learning-based enhancing network, identifies regions with relevant features. Then, a pan-tilt controller is responsible for guiding the light beam toward these areas, so that to provide information-rich images to the ego-motion estimation algorithm. Experimental results on a real robotic platform demonstrate the effectiveness of the proposed method, showing a reduction in the pose estimation error up to 75% with respect to a traditional fixed lighting technique.", "paper_authors": "Francesco Crocetti, Alberto Dionigi, Raffaele Brilli, Gabriele Costante, Paolo Valigi", "update_time": "2025-02-19", "comments": null, "paper_url": "http://arxiv.org/abs/2502.13708", "paper_id": "2502.13708", "code_url": null}}, "Visual Localization": {"2502.13803": {"paper_title": "3D Gaussian Splatting aided Localization for Large and Complex Indoor-Environments", "paper_abstract": "The field of visual localization has been researched for several decades and has meanwhile found many practical applications. Despite the strong progress in this field, there are still challenging situations in which established methods fail. We present an approach to significantly improve the accuracy and reliability of established visual localization methods by adding rendered images. In detail, we first use a modern visual SLAM approach that provides a 3D Gaussian Splatting (3DGS) based map to create reference data. We demonstrate that enriching reference data with images rendered from 3DGS at randomly sampled poses significantly improves the performance of both geometry-based visual localization and Scene Coordinate Regression (SCR) methods. Through comprehensive evaluation in a large industrial environment, we analyze the performance impact of incorporating these additional rendered views.", "paper_authors": "Vincent Ress, Jonas Meyer, Wei Zhang, David Skuddis, Uwe Soergel, Norbert Haala", "update_time": "2025-02-19", "comments": null, "paper_url": "http://arxiv.org/abs/2502.13803", "paper_id": "2502.13803", "code_url": null}}, "Keypoint Detection": {"2502.13484": {"paper_title": "2.5D U-Net with Depth Reduction for 3D CryoET Object Identification", "paper_abstract": "Cryo-electron tomography (cryoET) is a crucial technique for unveiling the structure of protein complexes. Automatically analyzing tomograms captured by cryoET is an essential step toward understanding cellular structures. In this paper, we introduce the 4th place solution from the CZII - CryoET Object Identification competition, which was organized to advance the development of automated tomogram analysis techniques. Our solution adopted a heatmap-based keypoint detection approach, utilizing an ensemble of two different types of 2.5D U-Net models with depth reduction. Despite its highly unified and simple architecture, our method achieved 4th place, demonstrating its effectiveness.", "paper_authors": "Yusuke Uchida, Takaaki Fukui", "update_time": "2025-02-19", "comments": null, "paper_url": "http://arxiv.org/abs/2502.13484", "paper_id": "2502.13484", "code_url": null}}, "NeRF": {"2502.13335": {"paper_title": "Geometry-Aware Diffusion Models for Multiview Scene Inpainting", "paper_abstract": "In this paper, we focus on 3D scene inpainting, where parts of an input image set, captured from different viewpoints, are masked out. The main challenge lies in generating plausible image completions that are geometrically consistent across views. Most recent work addresses this challenge by combining generative models with a 3D radiance field to fuse information across viewpoints. However, a major drawback of these methods is that they often produce blurry images due to the fusion of inconsistent cross-view images. To avoid blurry inpaintings, we eschew the use of an explicit or implicit radiance field altogether and instead fuse cross-view information in a learned space. In particular, we introduce a geometry-aware conditional generative model, capable of inpainting multi-view consistent images based on both geometric and appearance cues from reference images. A key advantage of our approach over existing methods is its unique ability to inpaint masked scenes with a limited number of views (i.e., few-view inpainting), whereas previous methods require relatively large image sets for their 3D model fitting step. Empirically, we evaluate and compare our scene-centric inpainting method on two datasets, SPIn-NeRF and NeRFiller, which contain images captured at narrow and wide baselines, respectively, and achieve state-of-the-art 3D inpainting performance on both. Additionally, we demonstrate the efficacy of our approach in the few-view setting compared to prior methods.", "paper_authors": "Ahmad Salimi, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Konstantinos G. Derpanis", "update_time": "2025-02-18", "comments": "Our project page is available at https://geomvi.github.io", "paper_url": "http://arxiv.org/abs/2502.13335", "paper_id": "2502.13335", "code_url": null}, "2502.13196": {"paper_title": "GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis", "paper_abstract": "Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics.", "paper_authors": "Pedro Martin, Ant\u00f3nio Rodrigues, Jo\u00e3o Ascenso, Maria Paula Queluz", "update_time": "2025-02-18", "comments": null, "paper_url": "http://arxiv.org/abs/2502.13196", "paper_id": "2502.13196", "code_url": null}}}