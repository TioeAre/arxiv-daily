{"SFM": {"2502.12545": {"paper_title": "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360$^\\circ$ Cameras", "paper_abstract": "We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D mapping and rendering of indoor environments. Traditional Structure-from-Motion (SfM) methods may not work well in large-scale indoor scenes due to the prevalence of textureless and repetitive regions. To overcome these challenges, our approach (IM360) leverages the wide field of view of omnidirectional images and integrates the spherical camera model into every core component of the SfM pipeline. In order to develop a comprehensive 3D reconstruction solution, we integrate a neural implicit surface reconstruction technique to generate high-quality surfaces from sparse input data. Additionally, we utilize a mesh-based neural rendering approach to refine texture maps and accurately capture view-dependent properties by combining diffuse and specular components. We evaluate our pipeline on large-scale indoor scenes from the Matterport3D and Stanford2D3D datasets. In practice, IM360 demonstrate superior performance in terms of textured mesh reconstruction over SOTA. We observe accuracy improvements in terms of camera localization and registration as well as rendering high frequency details.", "paper_authors": "Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha", "update_time": "2025-02-18", "comments": null, "paper_url": "http://arxiv.org/abs/2502.12545", "paper_id": "2502.12545", "code_url": null}}, "Visual Localization": {"2502.13146": {"paper_title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization", "paper_abstract": "The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in https://github.com/taco-group/Re-Align.", "paper_authors": "Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu", "update_time": "2025-02-18", "comments": "15 pages", "paper_url": "http://arxiv.org/abs/2502.13146", "paper_id": "2502.13146", "code_url": null}, "2502.12545": {"paper_title": "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360$^\\circ$ Cameras", "paper_abstract": "We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D mapping and rendering of indoor environments. Traditional Structure-from-Motion (SfM) methods may not work well in large-scale indoor scenes due to the prevalence of textureless and repetitive regions. To overcome these challenges, our approach (IM360) leverages the wide field of view of omnidirectional images and integrates the spherical camera model into every core component of the SfM pipeline. In order to develop a comprehensive 3D reconstruction solution, we integrate a neural implicit surface reconstruction technique to generate high-quality surfaces from sparse input data. Additionally, we utilize a mesh-based neural rendering approach to refine texture maps and accurately capture view-dependent properties by combining diffuse and specular components. We evaluate our pipeline on large-scale indoor scenes from the Matterport3D and Stanford2D3D datasets. In practice, IM360 demonstrate superior performance in terms of textured mesh reconstruction over SOTA. We observe accuracy improvements in terms of camera localization and registration as well as rendering high frequency details.", "paper_authors": "Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha", "update_time": "2025-02-18", "comments": null, "paper_url": "http://arxiv.org/abs/2502.12545", "paper_id": "2502.12545", "code_url": null}, "2502.12303": {"paper_title": "From Gaming to Research: GTA V for Synthetic Data Generation for Robotics and Navigations", "paper_abstract": "In computer vision, the development of robust algorithms capable of generalizing effectively in real-world scenarios more and more often requires large-scale datasets collected under diverse environmental conditions. However, acquiring such datasets is time-consuming, costly, and sometimes unfeasible. To address these limitations, the use of synthetic data has gained attention as a viable alternative, allowing researchers to generate vast amounts of data while simulating various environmental contexts in a controlled setting. In this study, we investigate the use of synthetic data in robotics and navigation, specifically focusing on Simultaneous Localization and Mapping (SLAM) and Visual Place Recognition (VPR). In particular, we introduce a synthetic dataset created using the virtual environment of the video game Grand Theft Auto V (GTA V), along with an algorithm designed to generate a VPR dataset, without human supervision. Through a series of experiments centered on SLAM and VPR, we demonstrate that synthetic data derived from GTA V are qualitatively comparable to real-world data. Furthermore, these synthetic data can complement or even substitute real-world data in these applications. This study sets the stage for the creation of large-scale synthetic datasets, offering a cost-effective and scalable solution for future research and development.", "paper_authors": "Matteo Scucchia, Matteo Ferrara, Davide Maltoni", "update_time": "2025-02-17", "comments": null, "paper_url": "http://arxiv.org/abs/2502.12303", "paper_id": "2502.12303", "code_url": null}}, "NeRF": {"2502.12673": {"paper_title": "ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by NeRFs Composition", "paper_abstract": "Efficient and accurate 3D reconstruction is essential for applications in cultural heritage. This study addresses the challenge of visualizing objects within large-scale scenes at a high level of detail (LOD) using Neural Radiance Fields (NeRFs). The aim is to improve the visual fidelity of chosen objects while maintaining the efficiency of the computations by focusing on details only for relevant content. The proposed ROI-NeRFs framework divides the scene into a Scene NeRF, which represents the overall scene at moderate detail, and multiple ROI NeRFs that focus on user-defined objects of interest. An object-focused camera selection module automatically groups relevant cameras for each NeRF training during the decomposition phase. In the composition phase, a Ray-level Compositional Rendering technique combines information from the Scene NeRF and ROI NeRFs, allowing simultaneous multi-object rendering composition. Quantitative and qualitative experiments conducted on two real-world datasets, including one on a complex eighteen's century cultural heritage room, demonstrate superior performance compared to baseline methods, improving LOD for object regions, minimizing artifacts, and without significantly increasing inference time.", "paper_authors": "Quoc-Anh Bui, Gilles Rougeron, G\u00e9raldine Morin, Simone Gasparini", "update_time": "2025-02-18", "comments": "17 pages including appendix, 16 figures, 8 tables", "paper_url": "http://arxiv.org/abs/2502.12673", "paper_id": "2502.12673", "code_url": null}}}