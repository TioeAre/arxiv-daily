{"SFM": {"2407.20219": {"paper_title": "Global Structure-from-Motion Revisited", "paper_abstract": "Recovering 3D structure and camera motion from images has been a long-standing focus of computer vision research and is known as Structure-from-Motion (SfM). Solutions to this problem are categorized into incremental and global approaches. Until now, the most popular systems follow the incremental paradigm due to its superior accuracy and robustness, while global approaches are drastically more scalable and efficient. With this work, we revisit the problem of global SfM and propose GLOMAP as a new general-purpose system that outperforms the state of the art in global SfM. In terms of accuracy and robustness, we achieve results on-par or superior to COLMAP, the most widely used incremental SfM, while being orders of magnitude faster. We share our system as an open-source implementation at {https://github.com/colmap/glomap}.", "paper_authors": "Linfei Pan, D\u00e1niel Bar\u00e1th, Marc Pollefeys, Johannes L. Sch\u00f6nberger", "update_time": "2024-07-29", "comments": "accepted at ECCV2024", "paper_url": "http://arxiv.org/abs/2407.20219", "paper_id": "2407.20219", "code_url": "https://github.com/colmap/glomap"}, "2407.19166": {"paper_title": "Revisit Self-supervised Depth Estimation with Local Structure-from-Motion", "paper_abstract": "Both self-supervised depth estimation and Structure-from-Motion (SfM) recover scene depth from RGB videos. Despite sharing a similar objective, the two approaches are disconnected. Prior works of self-supervision backpropagate losses defined within immediate neighboring frames. Instead of learning-through-loss, this work proposes an alternative scheme by performing local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and correspondence estimator to infer depthmaps and pair-wise correspondence maps. Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses and one depth adjustment for each depthmap. Finally, we fix camera poses and employ a NeRF, however, without a neural network, for dense triangulation and geometric verification. Poses, depth adjustments, and triangulated sparse depths are our outputs. For the first time, we show self-supervision within $5$ frames already benefits SoTA supervised depth and correspondence models.", "paper_authors": "Shengjie Zhu, Xiaoming Liu", "update_time": "2024-07-27", "comments": null, "paper_url": "http://arxiv.org/abs/2407.19166", "paper_id": "2407.19166", "code_url": null}}, "Image Matching": {"2407.19812": {"paper_title": "Image-text matching for large-scale book collections", "paper_abstract": "We address the problem of detecting and mapping all books in a collection of images to entries in a given book catalogue. Instead of performing independent retrieval for each book detected, we treat the image-text mapping problem as a many-to-many matching process, looking for the best overall match between the two sets. We combine a state-of-the-art segmentation method (SAM) to detect book spines and extract book information using a commercial OCR. We then propose a two-stage approach for text-image matching, where CLIP embeddings are used first for fast matching, followed by a second slower stage to refine the matching, employing either the Hungarian Algorithm or a BERT-based model trained to cope with noisy OCR input and partial text matches. To evaluate our approach, we publish a new dataset of annotated bookshelf images that covers the whole book collection of a public library in Spain. In addition, we provide two target lists of book metadata, a closed-set of 15k book titles that corresponds to the known library inventory, and an open-set of 2.3M book titles to simulate an open-world scenario. We report results on two settings, on one hand on a matching-only task, where the book segments and OCR is given and the objective is to perform many-to-many matching against the target lists, and a combined detection and matching task, where books must be first detected and recognised before they are matched to the target list entries. We show that both the Hungarian Matching and the proposed BERT-based model outperform a fuzzy string matching baseline, and we highlight inherent limitations of the matching algorithms as the target increases in size, and when either of the two sets (detected books or target book list) is incomplete. The dataset and code are available at https://github.com/llabres/library-dataset", "paper_authors": "Artemis Llabr\u00e9s, Arka Ujjal Dey, Dimosthenis Karatzas, Ernest Valveny", "update_time": "2024-07-29", "comments": null, "paper_url": "http://arxiv.org/abs/2407.19812", "paper_id": "2407.19812", "code_url": "https://github.com/llabres/library-dataset"}}, "NeRF": {"2407.20194": {"paper_title": "Radiance Fields for Robotic Teleoperation", "paper_abstract": "Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods. For videos and code, check out https://leggedrobotics.github.io/rffr.github.io/.", "paper_authors": "Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter", "update_time": "2024-07-29", "comments": "8 pages, 10 figures, Accepted to IROS 2024", "paper_url": "http://arxiv.org/abs/2407.20194", "paper_id": "2407.20194", "code_url": null}, "2407.19774": {"paper_title": "Garment Animation NeRF with Color Editing", "paper_abstract": "Generating high-fidelity garment animations through traditional workflows, from modeling to rendering, is both tedious and expensive. These workflows often require repetitive steps in response to updates in character motion, rendering viewpoint changes, or appearance edits. Although recent neural rendering offers an efficient solution for computationally intensive processes, it struggles with rendering complex garment animations containing fine wrinkle details and realistic garment-and-body occlusions, while maintaining structural consistency across frames and dense view rendering. In this paper, we propose a novel approach to directly synthesize garment animations from body motion sequences without the need for an explicit garment proxy. Our approach infers garment dynamic features from body motion, providing a preliminary overview of garment structure. Simultaneously, we capture detailed features from synthesized reference images of the garment's front and back, generated by a pre-trained image model. These features are then used to construct a neural radiance field that renders the garment animation video. Additionally, our technique enables garment recoloring by decomposing its visual elements. We demonstrate the generalizability of our method across unseen body motions and camera views, ensuring detailed structural consistency. Furthermore, we showcase its applicability to color editing on both real and synthetic garment data. Compared to existing neural rendering techniques, our method exhibits qualitative and quantitative improvements in garment dynamics and wrinkle detail modeling. Code is available at \\url{https://github.com/wrk226/GarmentAnimationNeRF}.", "paper_authors": "Renke Wang, Meng Zhang, Jun Li, Jian Yan", "update_time": "2024-07-29", "comments": null, "paper_url": "http://arxiv.org/abs/2407.19774", "paper_id": "2407.19774", "code_url": "https://github.com/wrk226/garmentanimationnerf"}, "2407.19166": {"paper_title": "Revisit Self-supervised Depth Estimation with Local Structure-from-Motion", "paper_abstract": "Both self-supervised depth estimation and Structure-from-Motion (SfM) recover scene depth from RGB videos. Despite sharing a similar objective, the two approaches are disconnected. Prior works of self-supervision backpropagate losses defined within immediate neighboring frames. Instead of learning-through-loss, this work proposes an alternative scheme by performing local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and correspondence estimator to infer depthmaps and pair-wise correspondence maps. Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses and one depth adjustment for each depthmap. Finally, we fix camera poses and employ a NeRF, however, without a neural network, for dense triangulation and geometric verification. Poses, depth adjustments, and triangulated sparse depths are our outputs. For the first time, we show self-supervision within $5$ frames already benefits SoTA supervised depth and correspondence models.", "paper_authors": "Shengjie Zhu, Xiaoming Liu", "update_time": "2024-07-27", "comments": null, "paper_url": "http://arxiv.org/abs/2407.19166", "paper_id": "2407.19166", "code_url": null}}}