{"Visual Localization": {"2504.11134": {"paper_title": "Visual Re-Ranking with Non-Visual Side Information", "paper_abstract": "The standard approach for visual place recognition is to use global image descriptors to retrieve the most similar database images for a given query image. The results can then be further improved with re-ranking methods that re-order the top scoring images. However, existing methods focus on re-ranking based on the same image descriptors that were used for the initial retrieval, which we argue provides limited additional signal.   In this work we propose Generalized Contextual Similarity Aggregation (GCSA), which is a graph neural network-based re-ranking method that, in addition to the visual descriptors, can leverage other types of available side information. This can for example be other sensor data (such as signal strength of nearby WiFi or BlueTooth endpoints) or geometric properties such as camera poses for database images. In many applications this information is already present or can be acquired with low effort. Our architecture leverages the concept of affinity vectors to allow for a shared encoding of the heterogeneous multi-modal input. Two large-scale datasets, covering both outdoor and indoor localization scenarios, are utilized for training and evaluation. In experiments we show significant improvement not only on image retrieval metrics, but also for the downstream visual localization task.", "paper_authors": "Gustav Hanning, Gabrielle Flood, Viktor Larsson", "update_time": "2025-04-15", "comments": "Accepted at Scandinavian Conference on Image Analysis (SCIA) 2025", "paper_url": "http://arxiv.org/abs/2504.11134", "paper_id": "2504.11134", "code_url": null}, "2504.10995": {"paper_title": "TMCIR: Token Merge Benefits Composed Image Retrieval", "paper_abstract": "Composed Image Retrieval (CIR) retrieves target images using a multi-modal query that combines a reference image with text describing desired modifications. The primary challenge is effectively fusing this visual and textual information. Current cross-modal feature fusion approaches for CIR exhibit an inherent bias in intention interpretation. These methods tend to disproportionately emphasize either the reference image features (visual-dominant fusion) or the textual modification intent (text-dominant fusion through image-to-text conversion). Such an imbalanced representation often fails to accurately capture and reflect the actual search intent of the user in the retrieval results. To address this challenge, we propose TMCIR, a novel framework that advances composed image retrieval through two key innovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP encoders contrastively using intent-reflecting pseudo-target images, synthesized from reference images and textual descriptions via a diffusion model. This step enhances the encoder ability of text to capture nuanced intents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune all encoders contrastively by comparing adaptive token-fusion features with the target image. This mechanism dynamically balances visual and textual representations within the contrastive learning pipeline, optimizing the composed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR datasets demonstrate that TMCIR significantly outperforms state-of-the-art methods, particularly in capturing nuanced user intent.", "paper_authors": "Chaoyang Wang, Zeyu Zhang, Long Teng, Zijun Li, Shichao Kan", "update_time": "2025-04-15", "comments": "arXiv admin note: text overlap with arXiv:2310.05473 by other authors", "paper_url": "http://arxiv.org/abs/2504.10995", "paper_id": "2504.10995", "code_url": null}}, "Keypoint Detection": {"2504.11063": {"paper_title": "UKDM: Underwater keypoint detection and matching using underwater image enhancement techniques", "paper_abstract": "The purpose of this paper is to explore the use of underwater image enhancement techniques to improve keypoint detection and matching. By applying advanced deep learning models, including generative adversarial networks and convolutional neural networks, we aim to find the best method which improves the accuracy of keypoint detection and the robustness of matching algorithms. We evaluate the performance of these techniques on various underwater datasets, demonstrating significant improvements over traditional methods.", "paper_authors": "Pedro Diaz-Garcia, Felix Escalona, Miguel Cazorla", "update_time": "2025-04-15", "comments": null, "paper_url": "http://arxiv.org/abs/2504.11063", "paper_id": "2504.11063", "code_url": null}, "2504.11031": {"paper_title": "Acquisition of high-quality images for camera calibration in robotics applications via speech prompts", "paper_abstract": "Accurate intrinsic and extrinsic camera calibration can be an important prerequisite for robotic applications that rely on vision as input. While there is ongoing research on enabling camera calibration using natural images, many systems in practice still rely on using designated calibration targets with e.g. checkerboard patterns or April tag grids. Once calibration images from different perspectives have been acquired and feature descriptors detected, those are typically used in an optimization process to minimize the geometric reprojection error. For this optimization to converge, input images need to be of sufficient quality and particularly sharpness; they should neither contain motion blur nor rolling-shutter artifacts that can arise when the calibration board was not static during image capture. In this work, we present a novel calibration image acquisition technique controlled via voice commands recorded with a clip-on microphone, that can be more robust and user-friendly than e.g. triggering capture with a remote control, or filtering out blurry frames from a video sequence in postprocessing. To achieve this, we use a state-of-the-art speech-to-text transcription model with accurate per-word timestamping to capture trigger words with precise temporal alignment. Our experiments show that the proposed method improves user experience by being fast and efficient, allowing us to successfully calibrate complex multi-camera setups.", "paper_authors": "Timm Linder, Kadir Yilmaz, David B. Adrian, Bastian Leibe", "update_time": "2025-04-15", "comments": "8 pages, 6 figures", "paper_url": "http://arxiv.org/abs/2504.11031", "paper_id": "2504.11031", "code_url": null}}}