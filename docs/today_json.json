{"SFM": {"2507.03306": {"paper_title": "MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion", "paper_abstract": "Multi-camera systems are increasingly vital in the environmental perception of autonomous vehicles and robotics. Their physical configuration offers inherent fixed relative pose constraints that benefit Structure-from-Motion (SfM). However, traditional global SfM systems struggle with robustness due to their optimization framework. We propose a novel global motion averaging framework for multi-camera systems, featuring two core components: a decoupled rotation averaging module and a hybrid translation averaging module. Our rotation averaging employs a hierarchical strategy by first estimating relative rotations within rigid camera units and then computing global rigid unit rotations. To enhance the robustness of translation averaging, we incorporate both camera-to-camera and camera-to-point constraints to initialize camera positions and 3D points with a convex distance-based objective function and refine them with an unbiased non-bilinear angle-based objective function. Experiments on large-scale datasets show that our system matches or exceeds incremental SfM accuracy while significantly improving efficiency. Our framework outperforms existing global SfM methods, establishing itself as a robust solution for real-world multi-camera SfM applications. The code is available at https://github.com/3dv-casia/MGSfM/.", "paper_authors": "Peilin Tao, Hainan Cui, Diantao Tu, Shuhan Shen", "update_time": "2025-07-04", "comments": "Accepted at ICCV 2025, The code is available at\n  https://github.com/3dv-casia/MGSfM/", "paper_url": "http://arxiv.org/abs/2507.03306", "paper_id": "2507.03306", "code_url": null}}, "Visual Localization": {"2507.04735": {"paper_title": "An analysis of vision-language models for fabric retrieval", "paper_abstract": "Effective cross-modal retrieval is essential for applications like information retrieval and recommendation systems, particularly in specialized domains such as manufacturing, where product information often consists of visual samples paired with a textual description. This paper investigates the use of Vision Language Models(VLMs) for zero-shot text-to-image retrieval on fabric samples. We address the lack of publicly available datasets by introducing an automated annotation pipeline that uses Multimodal Large Language Models (MLLMs) to generate two types of textual descriptions: freeform natural language and structured attribute-based descriptions. We produce these descriptions to evaluate retrieval performance across three Vision-Language Models: CLIP, LAION-CLIP, and Meta's Perception Encoder. Our experiments demonstrate that structured, attribute-rich descriptions significantly enhance retrieval accuracy, particularly for visually complex fabric classes, with the Perception Encoder outperforming other models due to its robust feature alignment capabilities. However, zero-shot retrieval remains challenging in this fine-grained domain, underscoring the need for domain-adapted approaches. Our findings highlight the importance of combining technical textual descriptions with advanced VLMs to optimize cross-modal retrieval in industrial applications.", "paper_authors": "Francesco Giuliari, Asif Khan Pattan, Mohamed Lamine Mekhalfi, Fabio Poiesi", "update_time": "2025-07-07", "comments": "Accepted at Ital-IA 2025", "paper_url": "http://arxiv.org/abs/2507.04735", "paper_id": "2507.04735", "code_url": null}, "2507.04667": {"paper_title": "What's Making That Sound Right Now? Video-centric Audio-Visual Localization", "paper_abstract": "Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization.", "paper_authors": "Hahyeon Choi, Junhoo Lee, Nojun Kwak", "update_time": "2025-07-07", "comments": "Published at ICCV 2025. Project page:\n  https://hahyeon610.github.io/Video-centric_Audio_Visual_Localization/", "paper_url": "http://arxiv.org/abs/2507.04667", "paper_id": "2507.04667", "code_url": null}, "2507.04662": {"paper_title": "Simultaneous Localization and Mapping Using Active mmWave Sensing in 5G NR", "paper_abstract": "Millimeter-wave (mmWave) 5G New Radio (NR) communication systems, with their high-resolution antenna arrays and extensive bandwidth, offer a transformative opportunity for high-throughput data transmission and advanced environmental sensing. Although passive sensing-based SLAM techniques can estimate user locations and environmental reflections simultaneously, their effectiveness is often constrained by assumptions of specular reflections and oversimplified map representations. To overcome these limitations, this work employs a mmWave 5G NR system for active sensing, enabling it to function similarly to a laser scanner for point cloud generation. Specifically, point clouds are extracted from the power delay profile estimated from each beam direction using a binary search approach. To ensure accuracy, hardware delays are calibrated with multiple predefined target points. Pose variations of the terminal are then estimated from point cloud data gathered along continuous trajectory viewpoints using point cloud registration algorithms. Loop closure detection and pose graph optimization are subsequently applied to refine the sensing results, achieving precise terminal localization and detailed radio map reconstruction. The system is implemented and validated through both simulations and experiments, confirming the effectiveness of the proposed approach.", "paper_authors": "Tao Du, Jie Yang, Fan Liu, Jiaxiang Guo, Shuqiang Xia, Chao-Kai Wen, Shi Jin", "update_time": "2025-07-07", "comments": "7 pages, 7 figures. Accepted for publication at the 2025 IEEE\n  International Conference on Communications (ICC). \\c{opyright} 2025 IEEE.\n  Personal use is permitted, but permission from IEEE must be obtained for all\n  other uses", "paper_url": "http://arxiv.org/abs/2507.04662", "paper_id": "2507.04662", "code_url": null}, "2507.04503": {"paper_title": "U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration", "paper_abstract": "Accurate localization using visual information is a critical yet challenging task, especially in urban environments where nearby buildings and construction sites significantly degrade GNSS (Global Navigation Satellite System) signal quality. This issue underscores the importance of visual localization techniques in scenarios where GNSS signals are unreliable. This paper proposes U-ViLAR, a novel uncertainty-aware visual localization framework designed to address these challenges while enabling adaptive localization using high-definition (HD) maps or navigation maps. Specifically, our method first extracts features from the input visual data and maps them into Bird's-Eye-View (BEV) space to enhance spatial consistency with the map input. Subsequently, we introduce: a) Perceptual Uncertainty-guided Association, which mitigates errors caused by perception uncertainty, and b) Localization Uncertainty-guided Registration, which reduces errors introduced by localization uncertainty. By effectively balancing the coarse-grained large-scale localization capability of association with the fine-grained precise localization capability of registration, our approach achieves robust and accurate localization. Experimental results demonstrate that our method achieves state-of-the-art performance across multiple localization tasks. Furthermore, our model has undergone rigorous testing on large-scale autonomous driving fleets and has demonstrated stable performance in various challenging urban scenarios.", "paper_authors": "Xiaofan Li, Zhihao Xu, Chenming Wu, Zhao Yang, Yumeng Zhang, Jiang-Jiang Liu, Haibao Yu, Fan Duan, Xiaoqing Ye, Yuan Wang, Shirui Li, Xun Sun, Ji Wan, Jun Wang", "update_time": "2025-07-06", "comments": "Vision Localization, Autonomous Driving, Bird's-Eye-View", "paper_url": "http://arxiv.org/abs/2507.04503", "paper_id": "2507.04503", "code_url": null}, "2507.03831": {"paper_title": "Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition", "paper_abstract": "Deep learning methods for Visual Place Recognition (VPR) have advanced significantly, largely driven by large-scale datasets. However, most existing approaches are trained on a single dataset, which can introduce dataset-specific inductive biases and limit model generalization. While multi-dataset joint training offers a promising solution for developing universal VPR models, divergences among training datasets can saturate limited information capacity in feature aggregation layers, leading to suboptimal performance. To address these challenges, we propose Query-based Adaptive Aggregation (QAA), a novel feature aggregation technique that leverages learned queries as reference codebooks to effectively enhance information capacity without significant computational or parameter complexity. We show that computing the Cross-query Similarity (CS) between query-level image features and reference codebooks provides a simple yet effective way to generate robust descriptors. Our results demonstrate that QAA outperforms state-of-the-art models, achieving balanced generalization across diverse datasets while maintaining peak performance comparable to dataset-specific models. Ablation studies further explore QAA's mechanisms and scalability. Visualizations reveal that the learned queries exhibit diverse attention patterns across datasets. Code will be publicly released.", "paper_authors": "Jiuhong Xiao, Yang Zhou, Giuseppe Loianno", "update_time": "2025-07-04", "comments": "9 pages, 4 figures", "paper_url": "http://arxiv.org/abs/2507.03831", "paper_id": "2507.03831", "code_url": null}}, "Image Matching": {"2507.03868": {"paper_title": "From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM", "paper_abstract": "In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval's capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios.", "paper_authors": "Xinyi Wu, Yanhao Jia, Luwei Xiao, Shuai Zhao, Fengkuang Chiang, Erik Cambria", "update_time": "2025-07-05", "comments": null, "paper_url": "http://arxiv.org/abs/2507.03868", "paper_id": "2507.03868", "code_url": null}}, "NeRF": {"2507.04408": {"paper_title": "A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields", "paper_abstract": "Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene representation and 3D recovery. To improve its performance on real-world data, depth regularizations have proven to be the most effective ones. However, depth estimation models not only require expensive 3D supervision in training, but also suffer from generalization issues. As a result, the depth estimations can be erroneous in practice, especially for outdoor unbounded scenes. In this paper, we propose to employ view-consistent distributions instead of fixed depth value estimations to regularize NeRF training. Specifically, the distribution is computed by utilizing both low-level color features and high-level distilled features from foundation models at the projected 2D pixel-locations from per-ray sampled 3D points. By sampling from the view-consistency distributions, an implicit regularization is imposed on the training of NeRF. We also utilize a depth-pushing loss that works in conjunction with the sampling technique to jointly provide effective regularizations for eliminating the failure modes. Extensive experiments conducted on various scenes from public datasets demonstrate that our proposed method can generate significantly better novel view synthesis results than state-of-the-art NeRF variants as well as different depth regularization methods.", "paper_authors": "Aoxiang Fan, Corentin Dumery, Nicolas Talabot, Pascal Fua", "update_time": "2025-07-06", "comments": "ICCV 2025 accepted", "paper_url": "http://arxiv.org/abs/2507.04408", "paper_id": "2507.04408", "code_url": null}}}