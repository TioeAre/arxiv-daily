{"Visual Localization": {"2406.07502": {"paper_title": "Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions", "paper_abstract": "Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed Image Textualization (IT), which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text. To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.", "paper_authors": "Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, Tong Zhang", "update_time": "2024-06-11", "comments": null, "paper_url": "http://arxiv.org/abs/2406.07502", "paper_id": "2406.07502", "code_url": "https://github.com/sterzhang/image-textualization"}, "2406.07450": {"paper_title": "Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning", "paper_abstract": "We perform a comprehensive benchmarking of contrastive frameworks for learning multimodal representations in the medical domain. Through this study, we aim to answer the following research questions: (i) How transferable are general-domain representations to the medical domain? (ii) Is multimodal contrastive training sufficient, or does it benefit from unimodal training as well? (iii) What is the impact of feature granularity on the effectiveness of multimodal medical representation learning? To answer these questions, we investigate eight contrastive learning approaches under identical training setups, and train them on 2.8 million image-text pairs from four datasets, and evaluate them on 25 downstream tasks, including classification (zero-shot and linear probing), image-to-text and text-to-image retrieval, and visual question-answering. Our findings suggest a positive answer to the first question, a negative answer to the second question, and the benefit of learning fine-grained features. Finally, we make our code publicly available.", "paper_authors": "Shuvendu Roy, Yasaman Parhizkar, Franklin Ogidi, Vahid Reza Khazaie, Michael Colacci, Ali Etemad, Elham Dolatabadi, Arash Afkanpour", "update_time": "2024-06-11", "comments": null, "paper_url": "http://arxiv.org/abs/2406.07450", "paper_id": "2406.07450", "code_url": "https://github.com/shuvenduroy/multimodal"}, "2406.07315": {"paper_title": "Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document Retrieval", "paper_abstract": "This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored for legislative historical document analysis systems, addressing the challenges of large-scale document retrieval in historical contexts. The benchmark comprises a vast repository of documents dating back to the XVII century, serving both as a training resource and an evaluation benchmark for retrieval systems. It fills a critical gap in the literature by focusing on complex extractive tasks within the domain of cultural heritage. The proposed benchmark tackles the multifaceted problem of historical document analysis, including text-to-image retrieval for queries and image-to-text topic extraction from document fragments, all while accommodating varying levels of document legibility. This benchmark aims to spur advancements in the field by providing baselines and data for the development and evaluation of robust historical document retrieval systems, particularly in scenarios characterized by wide historical spectrum.", "paper_authors": "Adri\u00e0 Molina, Oriol Ramos Terrades, Josep Llad\u00f3s", "update_time": "2024-06-11", "comments": "Preprint for the manuscript accepted for publication in the DAS2024\n  LNCS proceedings", "paper_url": "http://arxiv.org/abs/2406.07315", "paper_id": "2406.07315", "code_url": null}}, "NeRF": {"2406.07431": {"paper_title": "Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments", "paper_abstract": "We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground. We show that we can build a neural radiance field (NeRF) representation of the city -- online -- using RGB and depth images from different vantage points. This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets -- thereby giving a completely first-principles approach to actively tracking dynamic targets. We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps. This is slower than a greedy baseline which which does not use active perception. But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout's policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking.", "paper_authors": "Christopher D. Hsu, Pratik Chaudhari", "update_time": "2024-06-11", "comments": "8 pages, 8 figures, 1 table", "paper_url": "http://arxiv.org/abs/2406.07431", "paper_id": "2406.07431", "code_url": null}, "2406.06972": {"paper_title": "Generative Lifting of Multiview to 3D from Unknown Pose: Wrapping NeRF inside Diffusion", "paper_abstract": "We cast multiview reconstruction from unknown pose as a generative modeling problem. From a collection of unannotated 2D images of a scene, our approach simultaneously learns both a network to predict camera pose from 2D image input, as well as the parameters of a Neural Radiance Field (NeRF) for the 3D scene. To drive learning, we wrap both the pose prediction network and NeRF inside a Denoising Diffusion Probabilistic Model (DDPM) and train the system via the standard denoising objective. Our framework requires the system accomplish the task of denoising an input 2D image by predicting its pose and rendering the NeRF from that pose. Learning to denoise thus forces the system to concurrently learn the underlying 3D NeRF representation and a mapping from images to camera extrinsic parameters. To facilitate the latter, we design a custom network architecture to represent pose as a distribution, granting implicit capacity for discovering view correspondences when trained end-to-end for denoising alone. This technique allows our system to successfully build NeRFs, without pose knowledge, for challenging scenes where competing methods fail. At the conclusion of training, our learned NeRF can be extracted and used as a 3D scene model; our full system can be used to sample novel camera poses and generate novel-view images.", "paper_authors": "Xin Yuan, Rana Hanocka, Michael Maire", "update_time": "2024-06-11", "comments": null, "paper_url": "http://arxiv.org/abs/2406.06972", "paper_id": "2406.06972", "code_url": null}, "2406.06948": {"paper_title": "Neural Visibility Field for Uncertainty-Driven Active Mapping", "paper_abstract": "This paper presents Neural Visibility Field (NVF), a novel uncertainty quantification method for Neural Radiance Fields (NeRF) applied to active mapping. Our key insight is that regions not visible in the training views lead to inherently unreliable color predictions by NeRF at this region, resulting in increased uncertainty in the synthesized views. To address this, we propose to use Bayesian Networks to composite position-based field uncertainty into ray-based uncertainty in camera observations. Consequently, NVF naturally assigns higher uncertainty to unobserved regions, aiding robots to select the most informative next viewpoints. Extensive evaluations show that NVF excels not only in uncertainty quantification but also in scene reconstruction for active mapping, outperforming existing methods.", "paper_authors": "Shangjie Xue, Jesse Dill, Pranay Mathur, Frank Dellaert, Panagiotis Tsiotra, Danfei Xu", "update_time": "2024-06-11", "comments": "Accepted to CVPR 2024. More details can be found at\n  https://sites.google.com/view/nvf-cvpr24/", "paper_url": "http://arxiv.org/abs/2406.06948", "paper_id": "2406.06948", "code_url": null}}}